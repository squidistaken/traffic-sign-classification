{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0cbdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.1' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/David/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "%run \"../dataio/gtsrb_download.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c679b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataio.transforms import ToCompose, ToResize, ToRotate, ToNoise, ToTensor, ToNormalize\n",
    "from dataio.gtsrb_dataset import GTSRBDataset\n",
    "from dataio.dataloader import DataLoader\n",
    "from nn.layers.batchnorm2d import BatchNorm2D\n",
    "from nn.layers.conv2d import Conv2D\n",
    "from nn.layers.dropout import Dropout\n",
    "from nn.layers.flatten import Flatten\n",
    "from nn.layers.linear import Linear\n",
    "from nn.layers.maxpool2d import MaxPool2D\n",
    "from nn.layers.sequential import Sequential\n",
    "from nn.optim import Adam\n",
    "from nn.loss import cross_entropy\n",
    "from train import train\n",
    "from crossval import cross_validate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for training\n",
    "train_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToRotate(angle=15),\n",
    "    ToNoise(mean=0, std=0.05),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the transforms for validation and testing\n",
    "val_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Total number of entries in the dataset\n",
    "total_entries = 51839\n",
    "\n",
    "# Define the indices for each split\n",
    "def get_train_indices():\n",
    "    return list(range(int(0.7 * total_entries)))\n",
    "\n",
    "def get_val_indices():\n",
    "    start = int(0.7 * total_entries)\n",
    "    end = int(0.85 * total_entries)\n",
    "    return list(range(start, end))\n",
    "\n",
    "def get_test_indices():\n",
    "    start = int(0.85 * total_entries)\n",
    "    return list(range(start, total_entries))\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_train_indices(),\n",
    "    split=\"train\",\n",
    "    transforms=train_transforms\n",
    ")\n",
    "val_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_val_indices(),\n",
    "    split=\"val\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "test_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_test_indices(),\n",
    "    split=\"test\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf306bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example architecture without ReLU\n",
    "layers = [\n",
    "    Conv2D(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=32),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Conv2D(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=64),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Conv2D(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=128),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    Linear(in_features=128 * 8 * 8, out_features=512),  # Adjust input_size based on your image size and pooling layers\n",
    "    Dropout(p=0.5),\n",
    "    Linear(in_features=512, out_features=43)  # GTSRB has 43 classes\n",
    "]\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters from the model\n",
    "params = []\n",
    "for layer in model.layers:\n",
    "    param_list = layer.params()\n",
    "    for name, param in param_list:\n",
    "        params.append((layer, name, param))\n",
    "\n",
    "# Initialize the optimizer (e.g., SGD, Adam, or Momentum)\n",
    "optimizer = Adam(params, lr=0.001)  # Example using Adam optimizer\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model using the train function\n",
    "train_losses, val_losses, train_accs, val_accs = train(\n",
    "    model, train_loader, val_loader, loss_fn, optimizer, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(model, GTSRBDataset(), loss_fn, optimizer, log_dir='./logs/crossval', checkpoint_root='./checkpoints/crossval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e59078-fd11-4c3c-9310-9c6375ee1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses)\n",
    "print(val_losses)\n",
    "print(train_accs)\n",
    "print(val_accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
