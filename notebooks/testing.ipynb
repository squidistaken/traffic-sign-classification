{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 11:32:50,905 - INFO - GTSRB_Final_Training_Images not found, skipping deletion.\n",
      "2025-10-31 11:32:50,907 - INFO - GTSRB_Final_Test_Images not found, skipping deletion.\n",
      "2025-10-31 11:32:50,910 - INFO - GTSRB_Final_Test_GT not found, skipping deletion.\n",
      "2025-10-31 11:32:50,912 - INFO - GTSRB data is ready.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Kan geen bestand maken dat al bestaat: '../data/gtsrb/GTSRB_Final_Test_GT/GT-final_test.csv' -> '../data/gtsrb/labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\YEAR 3\\Neural Networks\\traffic-sign-classification\\dataio\\gtsrb_download.py:191\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    190\u001b[39m     get_data()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[43mclean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\YEAR 3\\Neural Networks\\traffic-sign-classification\\dataio\\gtsrb_download.py:113\u001b[39m, in \u001b[36mclean_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03mClean and organise the German Traffic Sign Recognition Benchmark\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03mdataset after downloading and unpacking. This involves:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m6. Deleting the `Final_Training` directory.\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# 1. Rename `GT_final_test.csv` to `labels.csv`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGTSRB_Final_Test_GT/GT-final_test.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabels.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    116\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m os.rmdir(DATA_DIR + \u001b[33m\"\u001b[39m\u001b[33mGTSRB_Final_Test_GT/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# 2. Create `images` directory if it doesn't exist\u001b[39;00m\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Kan geen bestand maken dat al bestaat: '../data/gtsrb/GTSRB_Final_Test_GT/GT-final_test.csv' -> '../data/gtsrb/labels.csv'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\Documents\\YEAR 3\\Neural Networks\\traffic-sign-classification\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "%run \"../dataio/gtsrb_download.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gebruiker\\Documents\\YEAR 3\\Neural Networks\\traffic-sign-classification\n"
     ]
    }
   ],
   "source": [
    "# Hard coded because I can't be bothered to make it dynamic LMAO\n",
    "%cd c:\\Users\\Gebruiker\\Documents\\YEAR 3\\Neural Networks\\traffic-sign-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c679b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataio.transforms import ToCompose, ToResize, ToRotate, ToNoise, ToTensor, ToNormalize\n",
    "from dataio.gtsrb_dataset import GTSRBDataset\n",
    "from dataio.dataloader import DataLoader\n",
    "from nn.layers.batchnorm2d import BatchNorm2D\n",
    "from nn.layers.conv2d import Conv2D\n",
    "from nn.layers.dropout import Dropout\n",
    "from nn.layers.flatten import Flatten\n",
    "from nn.layers.linear import Linear\n",
    "from nn.layers.maxpool2d import MaxPool2D\n",
    "from nn.layers.sequential import Sequential\n",
    "from nn.optim import Adam\n",
    "from nn.loss import cross_entropy\n",
    "from train import train\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0ac8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for training\n",
    "train_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToRotate(angle=15),\n",
    "    ToNoise(mean=0, std=0.05),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the transforms for validation and testing\n",
    "val_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load Data\n",
    "\n",
    "# %%\n",
    "# Total number of entries in the dataset\n",
    "total_entries = 51840\n",
    "\n",
    "# Define the indices for each split\n",
    "def get_train_indices():\n",
    "    return list(range(int(0.7 * total_entries)))\n",
    "\n",
    "def get_val_indices():\n",
    "    start = int(0.7 * total_entries)\n",
    "    end = int(0.85 * total_entries)\n",
    "    return list(range(start, end))\n",
    "\n",
    "def get_test_indices():\n",
    "    start = int(0.85 * total_entries)\n",
    "    return list(range(start, total_entries))\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_train_indices(),\n",
    "    split=\"train\",\n",
    "    transforms=train_transforms\n",
    ")\n",
    "val_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_val_indices(),\n",
    "    split=\"val\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "test_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_test_indices(),\n",
    "    split=\"test\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acf306bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example architecture without ReLU\n",
    "layers = [\n",
    "    Conv2D(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=32),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Conv2D(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=64),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Conv2D(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=128),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    Linear(in_features=128 * 8 * 8, out_features=512),  # Adjust input_size based on your image size and pooling layers\n",
    "    Dropout(p=0.5),\n",
    "    Linear(in_features=512, out_features=43)  # GTSRB has 43 classes\n",
    "]\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e3cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters from the model\n",
    "params = []\n",
    "for layer in model.layers:\n",
    "    param_list = layer.params()\n",
    "    for name, param in param_list:\n",
    "        params.append((layer, name, param))\n",
    "\n",
    "# Initialize the optimizer (e.g., SGD, Adam, or Momentum)\n",
    "optimizer = Adam(params, lr=0.001)  # Example using Adam optimizer\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model using the train function\n",
    "train_losses, val_losses, train_accs, val_accs = train(\n",
    "    model, train_loader, val_loader, loss_fn, optimizer, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e59078-fd11-4c3c-9310-9c6375ee1569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
