{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a0cbdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/traffic-sign-classification/dataio/gtsrb_download.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Initialize the logger\u001b[39;00m\n\u001b[32m      7\u001b[39m logger = Logger()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'logger'"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "%run \"../dataio/gtsrb_download.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad79e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'c:UsersGebruikerDocumentsYEAR 3Neural Networkstraffic-sign-classification'\n",
      "/home/marcusp/Documents/traffic-sign-classification/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Hard coded because I can't be bothered to make it dynamic LMAO\n",
    "%cd c:\\Users\\Gebruiker\\Documents\\YEAR 3\\Neural Networks\\traffic-sign-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c679b97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToCompose, ToResize, ToRotate, ToNoise, ToTensor, ToNormalize\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgtsrb_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GTSRBDataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mdataio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dataio'"
     ]
    }
   ],
   "source": [
    "from dataio.transforms import ToCompose, ToResize, ToRotate, ToNoise, ToTensor, ToNormalize\n",
    "from dataio.gtsrb_dataset import GTSRBDataset\n",
    "from dataio.dataloader import DataLoader\n",
    "from nn.layers.batchnorm2d import BatchNorm2D\n",
    "from nn.layers.conv2d import Conv2D\n",
    "from nn.layers.dropout import Dropout\n",
    "from nn.layers.flatten import Flatten\n",
    "from nn.layers.linear import Linear\n",
    "from nn.layers.maxpool2d import MaxPool2D\n",
    "from nn.layers.sequential import Sequential\n",
    "from nn.optim import Adam\n",
    "from nn.loss import cross_entropy\n",
    "from train import train\n",
    "from crossval import cross_validate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for training\n",
    "train_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToRotate(angle=15),\n",
    "    ToNoise(mean=0, std=0.05),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the transforms for validation and testing\n",
    "val_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the desired classes\n",
    "desired_classes = {0, 12, 17, 20, 30}\n",
    "\n",
    "# Create a temporary dataset instance to access all labels\n",
    "temp_dataset = GTSRBDataset(root=\"./data/gtsrb/\", split=\"train\", indices=[])\n",
    "all_labels = temp_dataset.labels_data  # This contains all the labels\n",
    "\n",
    "# Now filter the indices based on all_labels\n",
    "desired_indices = [i for i, (_, label) in enumerate(all_labels) if label in desired_classes]\n",
    "\n",
    "# Now define the splits based on desired_indices\n",
    "def get_train_indices():\n",
    "    return desired_indices[:int(0.7 * len(desired_indices))]\n",
    "\n",
    "def get_val_indices():\n",
    "    start = int(0.7 * len(desired_indices))\n",
    "    end = int(0.85 * len(desired_indices))\n",
    "    return desired_indices[start:end]\n",
    "\n",
    "def get_test_indices():\n",
    "    start = int(0.85 * len(desired_indices))\n",
    "    return desired_indices[start:]\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_train_indices(),\n",
    "    split=\"train\",\n",
    "    transforms=train_transforms\n",
    ")\n",
    "val_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_val_indices(),\n",
    "    split=\"val\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "test_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_test_indices(),\n",
    "    split=\"test\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf306bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example architecture without ReLU\n",
    "layers = [\n",
    "    Conv2D(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=32),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    Linear(in_features=32 * 32 * 32, out_features=512),  # Adjust input_size based on your image size and pooling layers\n",
    "    Dropout(p=0.5),\n",
    "    Linear(in_features=512, out_features=43)  # GTSRB has 43 classes\n",
    "]\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters from the model\n",
    "param_dict = {}\n",
    "for layer in model.layers:\n",
    "    param_list = layer.params()\n",
    "    if isinstance(param_list, dict):\n",
    "        param_dict.update(param_list)\n",
    "    elif isinstance(param_list, list):\n",
    "        for i, param in enumerate(param_list):\n",
    "            name = f\"layer_{layer}_{i}\"\n",
    "            param_dict[name] = param\n",
    "    else:\n",
    "        name = f\"layer_{layer}\"\n",
    "        param_dict[name] = param_list\n",
    "\n",
    "# Initialize the optimizer (e.g., SGD, Adam, or Momentum)\n",
    "optimizer = Adam(param_dict, lr=0.001)  # Example using Adam optimizer\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "# Train the model using the train function\n",
    "train_losses, val_losses, train_accs, val_accs = train(\n",
    "    model, train_loader, val_loader, loss_fn, optimizer, num_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e59078-fd11-4c3c-9310-9c6375ee1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validate(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    checkpoint_root=\"./checkpoints/crossval/\",\n",
    "    log_dir=\"./logs/crossval/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bb96a-7a46-4ba5-b176-79d5ed6bff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import evaluate\n",
    "\n",
    "importlib.reload(evaluate)  # reloads the updated file\n",
    "\n",
    "test_loss, test_acc = evaluate.evaluate(model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53314b89-e1bc-49a9-9e19-9f20f98aa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6c0f4-6896-4b5b-b477-90c193db7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import viz\n",
    "importlib.reload(viz)  # reloads the updated file\n",
    "\n",
    "\n",
    "viz.plot_curves(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd920e92-9ec9-4f4c-88b5-d0e04ffb3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for batch_data, batch_labels in test_loader:\n",
    "    predictions = np.argmax(model.forward(batch_data), axis=1)\n",
    "    viz.plot_confusion_matrix(batch_labels, predictions, desired_classes)\n",
    "    break  # Just plot the first batch for demonstration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f8ffb-a29f-4bc8-b517-42d7fdabb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(viz)\n",
    "# Compute and plot saliency maps one sample at a time\n",
    "for batch_data, batch_labels in test_loader:\n",
    "    for i, input_sample in enumerate(batch_data):\n",
    "        input_sample = np.array(input_sample, dtype=float)\n",
    "        saliency_map = compute_saliency_map(model, input_sample, batch_size=1000)  # Use batch_size as needed\n",
    "        plot_saliency_map(saliency_map)\n",
    "\n",
    "        if i >= 3:  # just first 4 samples for demonstration\n",
    "            break\n",
    "    break  # first batch only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152be61-fc91-4cfb-9daf-7ac5572f61cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
