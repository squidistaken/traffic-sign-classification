{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a0cbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 12:54:37,487 - INFO - Downloading GTSRB_Final_Training_Images...\n",
      "2025-11-02 12:55:37,532 - INFO - Unpacking GTSRB_Final_Training_Images...\n",
      "2025-11-02 12:55:42,981 - INFO - Downloading GTSRB_Final_Test_Images...\n",
      "2025-11-02 12:56:05,264 - INFO - Unpacking GTSRB_Final_Test_Images...\n",
      "2025-11-02 12:56:06,962 - INFO - Downloading GTSRB_Final_Test_GT...\n",
      "2025-11-02 12:56:07,093 - INFO - Unpacking GTSRB_Final_Test_GT...\n",
      "2025-11-02 12:56:07,096 - INFO - GTSRB data is ready.\n",
      "2025-11-02 12:56:08,708 - INFO - GTSRB data cleaned and organized.\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "%run \"../dataio/gtsrb_download.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad79e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marcusp/Documents/traffic-sign-classification\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c679b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataio.transforms import ToCompose, ToResize, ToRotate, ToNoise, ToTensor, ToNormalize\n",
    "from dataio.gtsrb_dataset import GTSRBDataset\n",
    "from dataio.dataloader import DataLoader\n",
    "from nn.layers.batchnorm2d import BatchNorm2D\n",
    "from nn.layers.conv2d import Conv2D\n",
    "from nn.layers.dropout import Dropout\n",
    "from nn.layers.flatten import Flatten\n",
    "from nn.layers.linear import Linear\n",
    "from nn.layers.maxpool2d import MaxPool2D\n",
    "from nn.layers.sequential import Sequential\n",
    "from nn.optim import Adam\n",
    "from nn.loss import cross_entropy\n",
    "from train import train\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ac8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms for training\n",
    "train_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToRotate(angle=15),\n",
    "    ToNoise(mean=0, std=0.05),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the transforms for validation and testing\n",
    "val_transforms = ToCompose([\n",
    "    ToResize(size=64),\n",
    "    ToTensor(),\n",
    "    ToNormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Define the desired classes\n",
    "desired_classes = {0, 12, 17, 20, 30}\n",
    "\n",
    "# Create a temporary dataset instance to access all labels\n",
    "temp_dataset = GTSRBDataset(root=\"./data/gtsrb/\", split=\"train\", indices=[])\n",
    "all_labels = temp_dataset.labels_data  # This contains all the labels\n",
    "\n",
    "# Now filter the indices based on all_labels\n",
    "desired_indices = [i for i, (_, label) in enumerate(all_labels) if label in desired_classes]\n",
    "\n",
    "# Now define the splits based on desired_indices\n",
    "def get_train_indices():\n",
    "    return desired_indices[:int(0.7 * len(desired_indices))]\n",
    "\n",
    "def get_val_indices():\n",
    "    start = int(0.7 * len(desired_indices))\n",
    "    end = int(0.85 * len(desired_indices))\n",
    "    return desired_indices[start:end]\n",
    "\n",
    "def get_test_indices():\n",
    "    start = int(0.85 * len(desired_indices))\n",
    "    return desired_indices[start:]\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_train_indices(),\n",
    "    split=\"train\",\n",
    "    transforms=train_transforms\n",
    ")\n",
    "val_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_val_indices(),\n",
    "    split=\"val\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "test_dataset = GTSRBDataset(\n",
    "    root=\"./data/gtsrb/\",\n",
    "    indices=get_test_indices(),\n",
    "    split=\"test\",\n",
    "    transforms=val_transforms\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf306bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example architecture without ReLU\n",
    "layers = [\n",
    "    Conv2D(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    BatchNorm2D(num_channels=32),\n",
    "    MaxPool2D(pool_size=2, stride=2),\n",
    "    Flatten(),\n",
    "    Linear(in_features=32 * 32 * 32, out_features=512),  # Adjust input_size based on your image size and pooling layers\n",
    "    Dropout(p=0.5),\n",
    "    Linear(in_features=512, out_features=43)  # GTSRB has 43 classes\n",
    "]\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e3cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters from the model\n",
    "param_dict = {}\n",
    "for layer in model.layers:\n",
    "    param_list = layer.params()\n",
    "    if isinstance(param_list, dict):\n",
    "        param_dict.update(param_list)\n",
    "    elif isinstance(param_list, list):\n",
    "        for i, param in enumerate(param_list):\n",
    "            name = f\"layer_{layer}_{i}\"\n",
    "            param_dict[name] = param\n",
    "    else:\n",
    "        name = f\"layer_{layer}\"\n",
    "        param_dict[name] = param_list\n",
    "\n",
    "# Initialize the optimizer (e.g., SGD, Adam, or Momentum)\n",
    "optimizer = Adam(param_dict, lr=0.001)  # Example using Adam optimizer\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = cross_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9991524b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 13:32:21,849 - DEBUG - Training started.\n",
      "2025-11-02 13:32:21,849 - root - DEBUG - Training started.\n",
      "2025-11-02 13:32:21,850 - DEBUG - Starting epoch 1/3.\n",
      "2025-11-02 13:32:21,850 - root - DEBUG - Starting epoch 1/3.\n",
      "2025-11-02 13:32:21,935 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:21,935 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:23,212 - DEBUG - Training batch loss: 7.402918635127451\n",
      "2025-11-02 13:32:23,212 - root - DEBUG - Training batch loss: 7.402918635127451\n",
      "2025-11-02 13:32:24,509 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:24,509 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:24,510 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:24,510 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:25,729 - DEBUG - Training batch loss: 6.742898953191796\n",
      "2025-11-02 13:32:25,729 - root - DEBUG - Training batch loss: 6.742898953191796\n",
      "2025-11-02 13:32:26,939 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:26,939 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:26,941 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:26,941 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:28,260 - DEBUG - Training batch loss: 7.177321112792853\n",
      "2025-11-02 13:32:28,260 - root - DEBUG - Training batch loss: 7.177321112792853\n",
      "2025-11-02 13:32:29,612 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:29,612 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:29,613 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:29,613 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:31,023 - DEBUG - Training batch loss: 7.318033795485691\n",
      "2025-11-02 13:32:31,023 - root - DEBUG - Training batch loss: 7.318033795485691\n",
      "2025-11-02 13:32:32,396 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:32,396 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:32,397 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:32,397 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:33,589 - DEBUG - Training batch loss: 6.911337822710194\n",
      "2025-11-02 13:32:33,589 - root - DEBUG - Training batch loss: 6.911337822710194\n",
      "2025-11-02 13:32:34,860 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:34,860 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:34,860 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:34,860 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:35,951 - DEBUG - Training batch loss: 7.751656793333857\n",
      "2025-11-02 13:32:35,951 - root - DEBUG - Training batch loss: 7.751656793333857\n",
      "2025-11-02 13:32:37,219 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:37,219 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:37,221 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:37,221 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:38,581 - DEBUG - Training batch loss: 7.108314286006329\n",
      "2025-11-02 13:32:38,581 - root - DEBUG - Training batch loss: 7.108314286006329\n",
      "2025-11-02 13:32:39,894 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:39,894 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:39,895 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:39,895 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:41,122 - DEBUG - Training batch loss: 7.102001411617168\n",
      "2025-11-02 13:32:41,122 - root - DEBUG - Training batch loss: 7.102001411617168\n",
      "2025-11-02 13:32:42,390 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:42,390 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:42,391 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:42,391 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:43,615 - DEBUG - Training batch loss: 6.9270535092220555\n",
      "2025-11-02 13:32:43,615 - root - DEBUG - Training batch loss: 6.9270535092220555\n",
      "2025-11-02 13:32:44,899 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:44,899 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:44,900 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:44,900 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:46,019 - DEBUG - Training batch loss: 6.5371328491393\n",
      "2025-11-02 13:32:46,019 - root - DEBUG - Training batch loss: 6.5371328491393\n",
      "2025-11-02 13:32:47,291 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:32:47,291 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:32:47,292 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:47,292 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:48,298 - DEBUG - Training batch loss: 7.24120748577505\n",
      "2025-11-02 13:32:48,298 - root - DEBUG - Training batch loss: 7.24120748577505\n",
      "2025-11-02 13:32:49,437 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:49,437 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:49,438 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:49,438 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:50,655 - DEBUG - Training batch loss: 6.8715725679595945\n",
      "2025-11-02 13:32:50,655 - root - DEBUG - Training batch loss: 6.8715725679595945\n",
      "2025-11-02 13:32:51,955 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:51,955 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:51,956 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:51,956 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:53,209 - DEBUG - Training batch loss: 6.989629491831147\n",
      "2025-11-02 13:32:53,209 - root - DEBUG - Training batch loss: 6.989629491831147\n",
      "2025-11-02 13:32:54,361 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:32:54,361 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:32:54,362 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:54,362 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:55,464 - DEBUG - Training batch loss: 7.144466433224438\n",
      "2025-11-02 13:32:55,464 - root - DEBUG - Training batch loss: 7.144466433224438\n",
      "2025-11-02 13:32:56,660 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:56,660 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:32:56,661 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:56,661 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:57,811 - DEBUG - Training batch loss: 8.02396874886747\n",
      "2025-11-02 13:32:57,811 - root - DEBUG - Training batch loss: 8.02396874886747\n",
      "2025-11-02 13:32:59,051 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:59,051 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:32:59,053 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:32:59,053 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:00,185 - DEBUG - Training batch loss: 7.005758150143622\n",
      "2025-11-02 13:33:00,185 - root - DEBUG - Training batch loss: 7.005758150143622\n",
      "2025-11-02 13:33:01,302 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:01,302 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:01,303 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:01,303 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:02,476 - DEBUG - Training batch loss: 7.496967052378906\n",
      "2025-11-02 13:33:02,476 - root - DEBUG - Training batch loss: 7.496967052378906\n",
      "2025-11-02 13:33:03,636 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:03,636 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:03,637 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:03,637 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:04,862 - DEBUG - Training batch loss: 7.299284719698202\n",
      "2025-11-02 13:33:04,862 - root - DEBUG - Training batch loss: 7.299284719698202\n",
      "2025-11-02 13:33:06,223 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:06,223 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:06,224 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:06,224 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:07,440 - DEBUG - Training batch loss: 6.895943073382281\n",
      "2025-11-02 13:33:07,440 - root - DEBUG - Training batch loss: 6.895943073382281\n",
      "2025-11-02 13:33:08,721 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:33:08,721 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:33:08,722 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:08,722 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:10,096 - DEBUG - Training batch loss: 6.889672300504895\n",
      "2025-11-02 13:33:10,096 - root - DEBUG - Training batch loss: 6.889672300504895\n",
      "2025-11-02 13:33:11,411 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:11,411 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:11,412 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:11,412 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:12,778 - DEBUG - Training batch loss: 6.62285295330938\n",
      "2025-11-02 13:33:12,778 - root - DEBUG - Training batch loss: 6.62285295330938\n",
      "2025-11-02 13:33:14,092 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:14,092 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:14,093 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:14,093 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:15,356 - DEBUG - Training batch loss: 7.225289203451862\n",
      "2025-11-02 13:33:15,356 - root - DEBUG - Training batch loss: 7.225289203451862\n",
      "2025-11-02 13:33:16,643 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:16,643 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:16,645 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:16,645 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:17,798 - DEBUG - Training batch loss: 8.060962819068793\n",
      "2025-11-02 13:33:17,798 - root - DEBUG - Training batch loss: 8.060962819068793\n",
      "2025-11-02 13:33:18,958 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:18,958 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:18,959 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:18,959 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:20,063 - DEBUG - Training batch loss: 7.1453662225281\n",
      "2025-11-02 13:33:20,063 - root - DEBUG - Training batch loss: 7.1453662225281\n",
      "2025-11-02 13:33:21,252 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:33:21,252 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:33:21,253 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:21,253 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:22,768 - DEBUG - Training batch loss: 6.706240913503574\n",
      "2025-11-02 13:33:22,768 - root - DEBUG - Training batch loss: 6.706240913503574\n",
      "2025-11-02 13:33:23,970 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:23,970 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:23,971 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:23,971 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:25,040 - DEBUG - Training batch loss: 6.675458502196774\n",
      "2025-11-02 13:33:25,040 - root - DEBUG - Training batch loss: 6.675458502196774\n",
      "2025-11-02 13:33:26,314 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:26,314 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:26,315 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:26,315 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:27,494 - DEBUG - Training batch loss: 7.026294874357211\n",
      "2025-11-02 13:33:27,494 - root - DEBUG - Training batch loss: 7.026294874357211\n",
      "2025-11-02 13:33:28,631 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:28,631 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:28,632 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:28,632 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:29,876 - DEBUG - Training batch loss: 8.023208078219655\n",
      "2025-11-02 13:33:29,876 - root - DEBUG - Training batch loss: 8.023208078219655\n",
      "2025-11-02 13:33:31,068 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:31,068 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:31,069 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:31,069 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:32,212 - DEBUG - Training batch loss: 7.039021102805302\n",
      "2025-11-02 13:33:32,212 - root - DEBUG - Training batch loss: 7.039021102805302\n",
      "2025-11-02 13:33:33,522 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:33,522 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:33,523 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:33,523 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:34,678 - DEBUG - Training batch loss: 7.764439239612301\n",
      "2025-11-02 13:33:34,678 - root - DEBUG - Training batch loss: 7.764439239612301\n",
      "2025-11-02 13:33:35,917 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:35,917 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:35,918 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:35,918 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:37,119 - DEBUG - Training batch loss: 7.639380792818712\n",
      "2025-11-02 13:33:37,119 - root - DEBUG - Training batch loss: 7.639380792818712\n",
      "2025-11-02 13:33:38,372 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:38,372 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:38,373 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:38,373 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:39,444 - DEBUG - Training batch loss: 7.023702863824279\n",
      "2025-11-02 13:33:39,444 - root - DEBUG - Training batch loss: 7.023702863824279\n",
      "2025-11-02 13:33:40,688 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:40,688 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:40,689 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:40,689 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:41,882 - DEBUG - Training batch loss: 7.155178292475975\n",
      "2025-11-02 13:33:41,882 - root - DEBUG - Training batch loss: 7.155178292475975\n",
      "2025-11-02 13:33:43,023 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:43,023 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:43,024 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:43,024 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:44,217 - DEBUG - Training batch loss: 6.82133024122034\n",
      "2025-11-02 13:33:44,217 - root - DEBUG - Training batch loss: 6.82133024122034\n",
      "2025-11-02 13:33:45,503 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:45,503 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:45,504 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:45,504 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:46,683 - DEBUG - Training batch loss: 6.712181158265203\n",
      "2025-11-02 13:33:46,683 - root - DEBUG - Training batch loss: 6.712181158265203\n",
      "2025-11-02 13:33:47,793 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:47,793 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:47,794 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:47,794 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:48,935 - DEBUG - Training batch loss: 8.317370358250805\n",
      "2025-11-02 13:33:48,935 - root - DEBUG - Training batch loss: 8.317370358250805\n",
      "2025-11-02 13:33:50,171 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:50,171 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:50,172 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:50,172 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:51,365 - DEBUG - Training batch loss: 7.435659154522817\n",
      "2025-11-02 13:33:51,365 - root - DEBUG - Training batch loss: 7.435659154522817\n",
      "2025-11-02 13:33:52,572 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:52,572 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:52,573 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:52,573 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:53,877 - DEBUG - Training batch loss: 7.261520303393759\n",
      "2025-11-02 13:33:53,877 - root - DEBUG - Training batch loss: 7.261520303393759\n",
      "2025-11-02 13:33:55,136 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:55,136 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:55,137 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:55,137 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:56,325 - DEBUG - Training batch loss: 8.093447992326446\n",
      "2025-11-02 13:33:56,325 - root - DEBUG - Training batch loss: 8.093447992326446\n",
      "2025-11-02 13:33:57,547 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:57,547 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:33:57,548 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:57,548 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:58,694 - DEBUG - Training batch loss: 6.772590976511902\n",
      "2025-11-02 13:33:58,694 - root - DEBUG - Training batch loss: 6.772590976511902\n",
      "2025-11-02 13:33:59,924 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:59,924 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:33:59,926 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:33:59,926 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:01,200 - DEBUG - Training batch loss: 7.7056373190215375\n",
      "2025-11-02 13:34:01,200 - root - DEBUG - Training batch loss: 7.7056373190215375\n",
      "2025-11-02 13:34:02,448 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:02,448 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:02,449 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:02,449 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:03,595 - DEBUG - Training batch loss: 6.88312746128814\n",
      "2025-11-02 13:34:03,595 - root - DEBUG - Training batch loss: 6.88312746128814\n",
      "2025-11-02 13:34:04,950 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:04,950 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:04,952 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:04,952 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:06,276 - DEBUG - Training batch loss: 7.326492947345473\n",
      "2025-11-02 13:34:06,276 - root - DEBUG - Training batch loss: 7.326492947345473\n",
      "2025-11-02 13:34:07,500 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:07,500 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:07,501 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:07,501 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:08,695 - DEBUG - Training batch loss: 7.245499714698442\n",
      "2025-11-02 13:34:08,695 - root - DEBUG - Training batch loss: 7.245499714698442\n",
      "2025-11-02 13:34:10,004 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:10,004 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:10,005 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:10,005 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:11,275 - DEBUG - Training batch loss: 7.522049874303342\n",
      "2025-11-02 13:34:11,275 - root - DEBUG - Training batch loss: 7.522049874303342\n",
      "2025-11-02 13:34:12,547 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:12,547 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:12,549 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:12,549 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:13,723 - DEBUG - Training batch loss: 7.265493429641686\n",
      "2025-11-02 13:34:13,723 - root - DEBUG - Training batch loss: 7.265493429641686\n",
      "2025-11-02 13:34:14,981 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:14,981 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:14,983 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:14,983 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:16,252 - DEBUG - Training batch loss: 6.838502623624812\n",
      "2025-11-02 13:34:16,252 - root - DEBUG - Training batch loss: 6.838502623624812\n",
      "2025-11-02 13:34:17,472 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:17,472 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:17,473 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:17,473 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:18,684 - DEBUG - Training batch loss: 7.726841665640307\n",
      "2025-11-02 13:34:18,684 - root - DEBUG - Training batch loss: 7.726841665640307\n",
      "2025-11-02 13:34:20,031 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:20,031 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:20,032 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:20,032 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:21,331 - DEBUG - Training batch loss: 7.444898820344793\n",
      "2025-11-02 13:34:21,331 - root - DEBUG - Training batch loss: 7.444898820344793\n",
      "2025-11-02 13:34:22,589 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:22,589 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:22,591 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:22,591 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:23,765 - DEBUG - Training batch loss: 7.372913479301414\n",
      "2025-11-02 13:34:23,765 - root - DEBUG - Training batch loss: 7.372913479301414\n",
      "2025-11-02 13:34:25,023 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:25,023 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:25,024 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:25,024 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:26,321 - DEBUG - Training batch loss: 7.3518909873932135\n",
      "2025-11-02 13:34:26,321 - root - DEBUG - Training batch loss: 7.3518909873932135\n",
      "2025-11-02 13:34:27,636 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:27,636 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:27,637 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:27,637 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:28,846 - DEBUG - Training batch loss: 6.788867671891727\n",
      "2025-11-02 13:34:28,846 - root - DEBUG - Training batch loss: 6.788867671891727\n",
      "2025-11-02 13:34:30,117 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:30,117 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:30,118 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:30,118 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:31,417 - DEBUG - Training batch loss: 6.834712117348392\n",
      "2025-11-02 13:34:31,417 - root - DEBUG - Training batch loss: 6.834712117348392\n",
      "2025-11-02 13:34:32,725 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:32,725 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:32,726 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:32,726 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:33,889 - DEBUG - Training batch loss: 6.528805825451781\n",
      "2025-11-02 13:34:33,889 - root - DEBUG - Training batch loss: 6.528805825451781\n",
      "2025-11-02 13:34:35,141 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:35,141 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:35,142 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:35,142 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:36,400 - DEBUG - Training batch loss: 6.84866801597509\n",
      "2025-11-02 13:34:36,400 - root - DEBUG - Training batch loss: 6.84866801597509\n",
      "2025-11-02 13:34:37,653 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:37,653 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:37,654 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:37,654 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:38,830 - DEBUG - Training batch loss: 7.186396328321539\n",
      "2025-11-02 13:34:38,830 - root - DEBUG - Training batch loss: 7.186396328321539\n",
      "2025-11-02 13:34:40,136 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:40,136 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:40,137 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:40,137 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:41,412 - DEBUG - Training batch loss: 7.002473341332591\n",
      "2025-11-02 13:34:41,412 - root - DEBUG - Training batch loss: 7.002473341332591\n",
      "2025-11-02 13:34:42,684 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:42,684 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:42,686 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:42,686 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:43,871 - DEBUG - Training batch loss: 6.6965627383457225\n",
      "2025-11-02 13:34:43,871 - root - DEBUG - Training batch loss: 6.6965627383457225\n",
      "2025-11-02 13:34:45,122 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:45,122 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:45,123 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:45,123 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:46,390 - DEBUG - Training batch loss: 7.035501612216072\n",
      "2025-11-02 13:34:46,390 - root - DEBUG - Training batch loss: 7.035501612216072\n",
      "2025-11-02 13:34:47,713 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:47,713 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:47,714 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:47,714 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:49,069 - DEBUG - Training batch loss: 6.8061082700536595\n",
      "2025-11-02 13:34:49,069 - root - DEBUG - Training batch loss: 6.8061082700536595\n",
      "2025-11-02 13:34:50,336 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:50,336 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:50,338 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:50,338 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:51,625 - DEBUG - Training batch loss: 7.675473518356751\n",
      "2025-11-02 13:34:51,625 - root - DEBUG - Training batch loss: 7.675473518356751\n",
      "2025-11-02 13:34:52,859 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:52,859 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:52,860 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:52,860 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:54,078 - DEBUG - Training batch loss: 8.106017696500011\n",
      "2025-11-02 13:34:54,078 - root - DEBUG - Training batch loss: 8.106017696500011\n",
      "2025-11-02 13:34:55,353 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:55,353 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:34:55,354 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:55,354 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:56,597 - DEBUG - Training batch loss: 7.775477227283094\n",
      "2025-11-02 13:34:56,597 - root - DEBUG - Training batch loss: 7.775477227283094\n",
      "2025-11-02 13:34:57,822 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:57,822 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:34:57,823 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:57,823 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:34:59,028 - DEBUG - Training batch loss: 6.334047998494066\n",
      "2025-11-02 13:34:59,028 - root - DEBUG - Training batch loss: 6.334047998494066\n",
      "2025-11-02 13:35:00,310 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:35:00,310 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:35:00,311 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:00,311 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:01,577 - DEBUG - Training batch loss: 7.728055044201227\n",
      "2025-11-02 13:35:01,577 - root - DEBUG - Training batch loss: 7.728055044201227\n",
      "2025-11-02 13:35:02,911 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:02,911 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:02,912 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:02,912 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:04,110 - DEBUG - Training batch loss: 7.5882395608243165\n",
      "2025-11-02 13:35:04,110 - root - DEBUG - Training batch loss: 7.5882395608243165\n",
      "2025-11-02 13:35:05,405 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:05,405 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:05,407 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:05,407 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:06,659 - DEBUG - Training batch loss: 7.1958338305796605\n",
      "2025-11-02 13:35:06,659 - root - DEBUG - Training batch loss: 7.1958338305796605\n",
      "2025-11-02 13:35:07,921 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:07,921 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:07,922 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:07,922 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:09,151 - DEBUG - Training batch loss: 7.499029232546579\n",
      "2025-11-02 13:35:09,151 - root - DEBUG - Training batch loss: 7.499029232546579\n",
      "2025-11-02 13:35:10,425 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:10,425 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:10,426 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:10,426 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:11,694 - DEBUG - Training batch loss: 7.296774158489082\n",
      "2025-11-02 13:35:11,694 - root - DEBUG - Training batch loss: 7.296774158489082\n",
      "2025-11-02 13:35:12,926 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:12,926 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:12,927 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:12,927 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:14,121 - DEBUG - Training batch loss: 7.286743997149814\n",
      "2025-11-02 13:35:14,121 - root - DEBUG - Training batch loss: 7.286743997149814\n",
      "2025-11-02 13:35:15,437 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:15,437 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:15,438 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:15,438 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:16,685 - DEBUG - Training batch loss: 7.2623002051652685\n",
      "2025-11-02 13:35:16,685 - root - DEBUG - Training batch loss: 7.2623002051652685\n",
      "2025-11-02 13:35:17,930 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:17,930 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:17,931 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:17,931 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:19,108 - DEBUG - Training batch loss: 6.619862619806302\n",
      "2025-11-02 13:35:19,108 - root - DEBUG - Training batch loss: 6.619862619806302\n",
      "2025-11-02 13:35:20,353 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:20,353 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:20,354 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:20,354 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:21,610 - DEBUG - Training batch loss: 6.750041665485011\n",
      "2025-11-02 13:35:21,610 - root - DEBUG - Training batch loss: 6.750041665485011\n",
      "2025-11-02 13:35:22,862 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:22,862 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:22,863 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:22,863 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:24,062 - DEBUG - Training batch loss: 6.385862084140806\n",
      "2025-11-02 13:35:24,062 - root - DEBUG - Training batch loss: 6.385862084140806\n",
      "2025-11-02 13:35:25,341 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:25,341 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:25,342 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:25,342 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:26,594 - DEBUG - Training batch loss: 8.072815395934406\n",
      "2025-11-02 13:35:26,594 - root - DEBUG - Training batch loss: 8.072815395934406\n",
      "2025-11-02 13:35:27,845 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:27,845 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:27,846 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:27,846 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:29,042 - DEBUG - Training batch loss: 6.7292115412618605\n",
      "2025-11-02 13:35:29,042 - root - DEBUG - Training batch loss: 6.7292115412618605\n",
      "2025-11-02 13:35:30,524 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:35:30,524 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:35:30,525 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:30,525 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:31,799 - DEBUG - Training batch loss: 7.792776573756898\n",
      "2025-11-02 13:35:31,799 - root - DEBUG - Training batch loss: 7.792776573756898\n",
      "2025-11-02 13:35:33,013 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:33,013 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:33,014 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:33,014 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:34,368 - DEBUG - Training batch loss: 7.26114072934606\n",
      "2025-11-02 13:35:34,368 - root - DEBUG - Training batch loss: 7.26114072934606\n",
      "2025-11-02 13:35:35,768 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:35,768 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:35,770 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:35,770 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:37,218 - DEBUG - Training batch loss: 6.639031772604248\n",
      "2025-11-02 13:35:37,218 - root - DEBUG - Training batch loss: 6.639031772604248\n",
      "2025-11-02 13:35:38,581 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:38,581 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:38,582 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:38,582 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:39,945 - DEBUG - Training batch loss: 7.443721417829733\n",
      "2025-11-02 13:35:39,945 - root - DEBUG - Training batch loss: 7.443721417829733\n",
      "2025-11-02 13:35:41,412 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:41,412 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:41,414 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:41,414 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:42,695 - DEBUG - Training batch loss: 7.006476900595215\n",
      "2025-11-02 13:35:42,695 - root - DEBUG - Training batch loss: 7.006476900595215\n",
      "2025-11-02 13:35:44,013 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:44,013 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:35:44,014 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:44,014 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:45,242 - DEBUG - Training batch loss: 7.043083605694559\n",
      "2025-11-02 13:35:45,242 - root - DEBUG - Training batch loss: 7.043083605694559\n",
      "2025-11-02 13:35:46,515 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:46,515 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:46,516 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:46,516 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:47,777 - DEBUG - Training batch loss: 7.232670591982566\n",
      "2025-11-02 13:35:47,777 - root - DEBUG - Training batch loss: 7.232670591982566\n",
      "2025-11-02 13:35:49,063 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:49,063 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:49,064 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:49,064 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:50,293 - DEBUG - Training batch loss: 7.3695178009319315\n",
      "2025-11-02 13:35:50,293 - root - DEBUG - Training batch loss: 7.3695178009319315\n",
      "2025-11-02 13:35:51,708 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:51,708 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:51,709 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:51,709 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:53,098 - DEBUG - Training batch loss: 6.412051928804804\n",
      "2025-11-02 13:35:53,098 - root - DEBUG - Training batch loss: 6.412051928804804\n",
      "2025-11-02 13:35:54,499 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:54,499 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:54,500 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:54,500 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:55,862 - DEBUG - Training batch loss: 7.516974945061396\n",
      "2025-11-02 13:35:55,862 - root - DEBUG - Training batch loss: 7.516974945061396\n",
      "2025-11-02 13:35:57,360 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:57,360 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:35:57,361 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:57,361 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:35:58,754 - DEBUG - Training batch loss: 6.691991708303834\n",
      "2025-11-02 13:35:58,754 - root - DEBUG - Training batch loss: 6.691991708303834\n",
      "2025-11-02 13:36:00,096 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:00,096 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:00,097 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:00,097 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:01,453 - DEBUG - Training batch loss: 6.21590872041694\n",
      "2025-11-02 13:36:01,453 - root - DEBUG - Training batch loss: 6.21590872041694\n",
      "2025-11-02 13:36:02,860 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:36:02,860 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:36:02,861 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:02,861 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:04,516 - DEBUG - Training batch loss: 7.877674327786453\n",
      "2025-11-02 13:36:04,516 - root - DEBUG - Training batch loss: 7.877674327786453\n",
      "2025-11-02 13:36:06,012 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:06,012 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:06,013 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:06,013 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:07,267 - DEBUG - Training batch loss: 6.593839769789282\n",
      "2025-11-02 13:36:07,267 - root - DEBUG - Training batch loss: 6.593839769789282\n",
      "2025-11-02 13:36:08,704 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:08,704 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:08,705 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:08,705 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:10,210 - DEBUG - Training batch loss: 7.319098931509512\n",
      "2025-11-02 13:36:10,210 - root - DEBUG - Training batch loss: 7.319098931509512\n",
      "2025-11-02 13:36:11,551 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:11,551 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:11,552 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:11,552 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:12,833 - DEBUG - Training batch loss: 7.920270533237131\n",
      "2025-11-02 13:36:12,833 - root - DEBUG - Training batch loss: 7.920270533237131\n",
      "2025-11-02 13:36:14,103 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:14,103 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:14,104 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:14,104 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:15,354 - DEBUG - Training batch loss: 7.643803359463799\n",
      "2025-11-02 13:36:15,354 - root - DEBUG - Training batch loss: 7.643803359463799\n",
      "2025-11-02 13:36:16,567 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:16,567 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:16,568 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:16,568 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:17,763 - DEBUG - Training batch loss: 7.611977287101739\n",
      "2025-11-02 13:36:17,763 - root - DEBUG - Training batch loss: 7.611977287101739\n",
      "2025-11-02 13:36:19,059 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:19,059 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:19,059 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:19,059 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:20,355 - DEBUG - Training batch loss: 7.570732715267082\n",
      "2025-11-02 13:36:20,355 - root - DEBUG - Training batch loss: 7.570732715267082\n",
      "2025-11-02 13:36:21,601 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:21,601 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:21,602 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:21,602 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:22,878 - DEBUG - Training batch loss: 6.973526442197977\n",
      "2025-11-02 13:36:22,878 - root - DEBUG - Training batch loss: 6.973526442197977\n",
      "2025-11-02 13:36:24,265 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:24,265 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:24,266 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:24,266 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:25,536 - DEBUG - Training batch loss: 6.97939969451743\n",
      "2025-11-02 13:36:25,536 - root - DEBUG - Training batch loss: 6.97939969451743\n",
      "2025-11-02 13:36:26,754 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:26,754 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:26,755 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:26,755 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:27,989 - DEBUG - Training batch loss: 7.407069187093525\n",
      "2025-11-02 13:36:27,989 - root - DEBUG - Training batch loss: 7.407069187093525\n",
      "2025-11-02 13:36:29,269 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:29,269 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:29,270 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:29,270 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:30,747 - DEBUG - Training batch loss: 7.3773470809175095\n",
      "2025-11-02 13:36:30,747 - root - DEBUG - Training batch loss: 7.3773470809175095\n",
      "2025-11-02 13:36:32,066 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:32,066 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:32,067 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:32,067 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:33,290 - DEBUG - Training batch loss: 6.8383924036789905\n",
      "2025-11-02 13:36:33,290 - root - DEBUG - Training batch loss: 6.8383924036789905\n",
      "2025-11-02 13:36:34,581 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:34,581 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:34,582 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:34,582 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:35,916 - DEBUG - Training batch loss: 7.3809873868214115\n",
      "2025-11-02 13:36:35,916 - root - DEBUG - Training batch loss: 7.3809873868214115\n",
      "2025-11-02 13:36:37,196 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:37,196 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:37,198 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:37,198 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:38,385 - DEBUG - Training batch loss: 7.339199631171672\n",
      "2025-11-02 13:36:38,385 - root - DEBUG - Training batch loss: 7.339199631171672\n",
      "2025-11-02 13:36:39,634 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:39,634 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:39,635 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:39,635 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:40,890 - DEBUG - Training batch loss: 7.704381813486608\n",
      "2025-11-02 13:36:40,890 - root - DEBUG - Training batch loss: 7.704381813486608\n",
      "2025-11-02 13:36:42,153 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:42,153 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:42,154 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:42,154 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:43,324 - DEBUG - Training batch loss: 7.0806921336159165\n",
      "2025-11-02 13:36:43,324 - root - DEBUG - Training batch loss: 7.0806921336159165\n",
      "2025-11-02 13:36:44,639 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:44,639 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:44,641 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:44,641 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:46,009 - DEBUG - Training batch loss: 7.262930303482365\n",
      "2025-11-02 13:36:46,009 - root - DEBUG - Training batch loss: 7.262930303482365\n",
      "2025-11-02 13:36:47,363 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:47,363 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:47,364 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:47,364 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:48,571 - DEBUG - Training batch loss: 6.999591447958959\n",
      "2025-11-02 13:36:48,571 - root - DEBUG - Training batch loss: 6.999591447958959\n",
      "2025-11-02 13:36:49,835 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:49,835 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:36:49,836 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:49,836 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:51,117 - DEBUG - Training batch loss: 8.032193637208296\n",
      "2025-11-02 13:36:51,117 - root - DEBUG - Training batch loss: 8.032193637208296\n",
      "2025-11-02 13:36:52,360 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:52,360 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:52,361 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:52,361 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:53,633 - DEBUG - Training batch loss: 7.300531393140798\n",
      "2025-11-02 13:36:53,633 - root - DEBUG - Training batch loss: 7.300531393140798\n",
      "2025-11-02 13:36:55,004 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:55,004 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:36:55,005 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:55,005 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:56,329 - DEBUG - Training batch loss: 6.60023201710748\n",
      "2025-11-02 13:36:56,329 - root - DEBUG - Training batch loss: 6.60023201710748\n",
      "2025-11-02 13:36:57,567 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:36:57,567 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:36:57,568 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:57,568 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:36:58,765 - DEBUG - Training batch loss: 7.265606148738014\n",
      "2025-11-02 13:36:58,765 - root - DEBUG - Training batch loss: 7.265606148738014\n",
      "2025-11-02 13:37:00,077 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:00,077 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:00,078 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:00,078 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:01,309 - DEBUG - Training batch loss: 7.92422495170719\n",
      "2025-11-02 13:37:01,309 - root - DEBUG - Training batch loss: 7.92422495170719\n",
      "2025-11-02 13:37:02,594 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:02,594 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:02,595 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:02,595 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:03,874 - DEBUG - Training batch loss: 7.346099987786435\n",
      "2025-11-02 13:37:03,874 - root - DEBUG - Training batch loss: 7.346099987786435\n",
      "2025-11-02 13:37:05,122 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:05,122 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:05,123 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:05,123 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:06,372 - DEBUG - Training batch loss: 7.677701558544532\n",
      "2025-11-02 13:37:06,372 - root - DEBUG - Training batch loss: 7.677701558544532\n",
      "2025-11-02 13:37:07,747 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:07,747 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:07,748 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:07,748 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:09,008 - DEBUG - Training batch loss: 7.4069994452845895\n",
      "2025-11-02 13:37:09,008 - root - DEBUG - Training batch loss: 7.4069994452845895\n",
      "2025-11-02 13:37:10,450 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:10,450 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:10,452 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:10,452 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:11,759 - DEBUG - Training batch loss: 7.2720605495899875\n",
      "2025-11-02 13:37:11,759 - root - DEBUG - Training batch loss: 7.2720605495899875\n",
      "2025-11-02 13:37:13,087 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:13,087 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:13,088 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:13,088 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:14,377 - DEBUG - Training batch loss: 7.362385722337461\n",
      "2025-11-02 13:37:14,377 - root - DEBUG - Training batch loss: 7.362385722337461\n",
      "2025-11-02 13:37:15,877 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:15,877 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:15,878 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:15,878 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:17,147 - DEBUG - Training batch loss: 7.430498046836823\n",
      "2025-11-02 13:37:17,147 - root - DEBUG - Training batch loss: 7.430498046836823\n",
      "2025-11-02 13:37:18,382 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:18,382 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:18,384 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:18,384 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:19,586 - DEBUG - Training batch loss: 7.117851151625186\n",
      "2025-11-02 13:37:19,586 - root - DEBUG - Training batch loss: 7.117851151625186\n",
      "2025-11-02 13:37:20,929 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:20,929 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:20,930 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:20,930 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:22,176 - DEBUG - Training batch loss: 6.64757422630117\n",
      "2025-11-02 13:37:22,176 - root - DEBUG - Training batch loss: 6.64757422630117\n",
      "2025-11-02 13:37:23,458 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:23,458 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:23,459 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:23,459 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:24,728 - DEBUG - Training batch loss: 6.948091093781289\n",
      "2025-11-02 13:37:24,728 - root - DEBUG - Training batch loss: 6.948091093781289\n",
      "2025-11-02 13:37:26,015 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:26,015 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:26,016 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:26,016 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:27,345 - DEBUG - Training batch loss: 8.040128717701169\n",
      "2025-11-02 13:37:27,345 - root - DEBUG - Training batch loss: 8.040128717701169\n",
      "2025-11-02 13:37:28,788 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:28,788 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:28,789 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:28,789 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:30,072 - DEBUG - Training batch loss: 7.8173506378029955\n",
      "2025-11-02 13:37:30,072 - root - DEBUG - Training batch loss: 7.8173506378029955\n",
      "2025-11-02 13:37:31,327 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:31,327 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:37:31,327 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:31,327 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:37:32,599 - DEBUG - Training batch loss: 6.78214789401709\n",
      "2025-11-02 13:37:32,599 - root - DEBUG - Training batch loss: 6.78214789401709\n",
      "2025-11-02 13:37:33,909 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:33,909 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:37:33,910 - INFO - Epoch 1/3, Train Loss: 7.2240,Train Accuracy: 0.0140\n",
      "2025-11-02 13:37:33,910 - root - INFO - Epoch 1/3, Train Loss: 7.2240,Train Accuracy: 0.0140\n",
      "2025-11-02 13:37:33,957 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:33,957 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:35,556 - DEBUG - Validation batch loss: 14.983504516114152\n",
      "2025-11-02 13:37:35,556 - root - DEBUG - Validation batch loss: 14.983504516114152\n",
      "2025-11-02 13:37:35,560 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:35,560 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:35,562 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:35,562 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:36,854 - DEBUG - Validation batch loss: 25.909096817062746\n",
      "2025-11-02 13:37:36,854 - root - DEBUG - Validation batch loss: 25.909096817062746\n",
      "2025-11-02 13:37:36,856 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:36,856 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:36,859 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:36,859 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:38,415 - DEBUG - Validation batch loss: 27.4003511735109\n",
      "2025-11-02 13:37:38,415 - root - DEBUG - Validation batch loss: 27.4003511735109\n",
      "2025-11-02 13:37:38,419 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:38,419 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:38,421 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:38,421 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:39,583 - DEBUG - Validation batch loss: 27.4003511735109\n",
      "2025-11-02 13:37:39,583 - root - DEBUG - Validation batch loss: 27.4003511735109\n",
      "2025-11-02 13:37:39,589 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:39,589 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:39,590 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:39,590 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:40,787 - DEBUG - Validation batch loss: 26.982853219924266\n",
      "2025-11-02 13:37:40,787 - root - DEBUG - Validation batch loss: 26.982853219924266\n",
      "2025-11-02 13:37:40,787 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:40,787 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:40,788 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:40,788 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:41,951 - DEBUG - Validation batch loss: 13.93631211527589\n",
      "2025-11-02 13:37:41,951 - root - DEBUG - Validation batch loss: 13.93631211527589\n",
      "2025-11-02 13:37:41,952 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:41,952 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:41,953 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:41,953 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:43,122 - DEBUG - Validation batch loss: 9.670682731929592\n",
      "2025-11-02 13:37:43,122 - root - DEBUG - Validation batch loss: 9.670682731929592\n",
      "2025-11-02 13:37:43,123 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:43,123 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:43,123 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:43,123 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:44,338 - DEBUG - Validation batch loss: 9.532857187496283\n",
      "2025-11-02 13:37:44,338 - root - DEBUG - Validation batch loss: 9.532857187496283\n",
      "2025-11-02 13:37:44,340 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:44,340 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:44,342 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:44,342 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:45,569 - DEBUG - Validation batch loss: 7.734300193052655\n",
      "2025-11-02 13:37:45,569 - root - DEBUG - Validation batch loss: 7.734300193052655\n",
      "2025-11-02 13:37:45,570 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:45,570 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:45,571 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:45,571 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:46,675 - DEBUG - Validation batch loss: 21.22665325771607\n",
      "2025-11-02 13:37:46,675 - root - DEBUG - Validation batch loss: 21.22665325771607\n",
      "2025-11-02 13:37:46,676 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:46,676 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:46,677 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:46,677 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:47,874 - DEBUG - Validation batch loss: 11.891771643683697\n",
      "2025-11-02 13:37:47,874 - root - DEBUG - Validation batch loss: 11.891771643683697\n",
      "2025-11-02 13:37:47,876 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:47,876 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:47,878 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:47,878 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:49,124 - DEBUG - Validation batch loss: 16.3422981456548\n",
      "2025-11-02 13:37:49,124 - root - DEBUG - Validation batch loss: 16.3422981456548\n",
      "2025-11-02 13:37:49,128 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:49,128 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:49,130 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:49,130 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:50,309 - DEBUG - Validation batch loss: 31.962781160768923\n",
      "2025-11-02 13:37:50,309 - root - DEBUG - Validation batch loss: 31.962781160768923\n",
      "2025-11-02 13:37:50,311 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:50,311 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:50,311 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:50,311 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:51,514 - DEBUG - Validation batch loss: 13.257320669369552\n",
      "2025-11-02 13:37:51,514 - root - DEBUG - Validation batch loss: 13.257320669369552\n",
      "2025-11-02 13:37:51,516 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:51,516 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:51,520 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:51,520 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:52,707 - DEBUG - Validation batch loss: 19.17290884047694\n",
      "2025-11-02 13:37:52,707 - root - DEBUG - Validation batch loss: 19.17290884047694\n",
      "2025-11-02 13:37:52,708 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:52,708 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:52,717 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:52,717 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:53,897 - DEBUG - Validation batch loss: 34.37967555567262\n",
      "2025-11-02 13:37:53,897 - root - DEBUG - Validation batch loss: 34.37967555567262\n",
      "2025-11-02 13:37:53,898 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:53,898 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:53,899 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:53,899 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:55,108 - DEBUG - Validation batch loss: 13.847588007625308\n",
      "2025-11-02 13:37:55,108 - root - DEBUG - Validation batch loss: 13.847588007625308\n",
      "2025-11-02 13:37:55,111 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:55,111 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:55,116 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:55,116 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:56,292 - DEBUG - Validation batch loss: 12.773835271567231\n",
      "2025-11-02 13:37:56,292 - root - DEBUG - Validation batch loss: 12.773835271567231\n",
      "2025-11-02 13:37:56,293 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:56,293 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:56,294 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:56,294 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:57,568 - DEBUG - Validation batch loss: 11.05792634801172\n",
      "2025-11-02 13:37:57,568 - root - DEBUG - Validation batch loss: 11.05792634801172\n",
      "2025-11-02 13:37:57,569 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:57,569 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:57,570 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:57,570 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:58,791 - DEBUG - Validation batch loss: 14.752692871880742\n",
      "2025-11-02 13:37:58,791 - root - DEBUG - Validation batch loss: 14.752692871880742\n",
      "2025-11-02 13:37:58,793 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:58,793 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:58,795 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:58,795 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:59,989 - DEBUG - Validation batch loss: 19.733302573296175\n",
      "2025-11-02 13:37:59,989 - root - DEBUG - Validation batch loss: 19.733302573296175\n",
      "2025-11-02 13:37:59,990 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:59,990 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:37:59,991 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:37:59,991 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:01,115 - DEBUG - Validation batch loss: 21.241361603855964\n",
      "2025-11-02 13:38:01,115 - root - DEBUG - Validation batch loss: 21.241361603855964\n",
      "2025-11-02 13:38:01,116 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:01,116 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:01,131 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:01,131 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:02,353 - DEBUG - Validation batch loss: 38.1263882378467\n",
      "2025-11-02 13:38:02,353 - root - DEBUG - Validation batch loss: 38.1263882378467\n",
      "2025-11-02 13:38:02,354 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:02,354 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:02,355 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:02,355 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:03,581 - DEBUG - Validation batch loss: 20.032209523016956\n",
      "2025-11-02 13:38:03,581 - root - DEBUG - Validation batch loss: 20.032209523016956\n",
      "2025-11-02 13:38:03,582 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:03,582 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:03,583 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:03,583 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:04,763 - DEBUG - Validation batch loss: 22.16523795927668\n",
      "2025-11-02 13:38:04,763 - root - DEBUG - Validation batch loss: 22.16523795927668\n",
      "2025-11-02 13:38:04,768 - DEBUG - Validation batch accuracy: 0.03125\n",
      "2025-11-02 13:38:04,768 - root - DEBUG - Validation batch accuracy: 0.03125\n",
      "2025-11-02 13:38:04,769 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:04,769 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:05,945 - DEBUG - Validation batch loss: 16.719086300696212\n",
      "2025-11-02 13:38:05,945 - root - DEBUG - Validation batch loss: 16.719086300696212\n",
      "2025-11-02 13:38:05,946 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:05,946 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:05,947 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:05,947 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:38:07,203 - DEBUG - Validation batch loss: 45.74114674950775\n",
      "2025-11-02 13:38:07,203 - root - DEBUG - Validation batch loss: 45.74114674950775\n",
      "2025-11-02 13:38:07,204 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:07,204 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:38:07,207 - INFO - Epoch 1/3,Val Loss: 20.2954, Val Accuracy: 0.0012\n",
      "2025-11-02 13:38:07,207 - root - INFO - Epoch 1/3,Val Loss: 20.2954, Val Accuracy: 0.0012\n",
      "2025-11-02 13:38:07,463 - DEBUG - Checkpoint saved to checkpoints/model_epoch_1.npz\n",
      "2025-11-02 13:38:07,463 - root - DEBUG - Checkpoint saved to checkpoints/model_epoch_1.npz\n",
      "2025-11-02 13:38:07,463 - DEBUG - Starting epoch 2/3.\n",
      "2025-11-02 13:38:07,463 - root - DEBUG - Starting epoch 2/3.\n",
      "2025-11-02 13:38:07,538 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:07,538 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:09,102 - DEBUG - Training batch loss: 7.787105315118992\n",
      "2025-11-02 13:38:09,102 - root - DEBUG - Training batch loss: 7.787105315118992\n",
      "2025-11-02 13:38:10,431 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:10,431 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:10,433 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:10,433 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:11,702 - DEBUG - Training batch loss: 7.138382555865055\n",
      "2025-11-02 13:38:11,702 - root - DEBUG - Training batch loss: 7.138382555865055\n",
      "2025-11-02 13:38:12,952 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:12,952 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:12,953 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:12,953 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:14,183 - DEBUG - Training batch loss: 6.478648227262752\n",
      "2025-11-02 13:38:14,183 - root - DEBUG - Training batch loss: 6.478648227262752\n",
      "2025-11-02 13:38:15,478 - DEBUG - Training batch accuracy: 0.09375\n",
      "2025-11-02 13:38:15,478 - root - DEBUG - Training batch accuracy: 0.09375\n",
      "2025-11-02 13:38:15,480 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:15,480 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:16,924 - DEBUG - Training batch loss: 6.468225834962118\n",
      "2025-11-02 13:38:16,924 - root - DEBUG - Training batch loss: 6.468225834962118\n",
      "2025-11-02 13:38:18,335 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:18,335 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:18,336 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:18,336 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:19,584 - DEBUG - Training batch loss: 7.129082213837191\n",
      "2025-11-02 13:38:19,584 - root - DEBUG - Training batch loss: 7.129082213837191\n",
      "2025-11-02 13:38:20,921 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:20,921 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:20,922 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:20,922 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:22,330 - DEBUG - Training batch loss: 6.956486283211943\n",
      "2025-11-02 13:38:22,330 - root - DEBUG - Training batch loss: 6.956486283211943\n",
      "2025-11-02 13:38:23,718 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:23,718 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:23,719 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:23,719 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:25,077 - DEBUG - Training batch loss: 8.00788963715547\n",
      "2025-11-02 13:38:25,077 - root - DEBUG - Training batch loss: 8.00788963715547\n",
      "2025-11-02 13:38:26,460 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:26,460 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:26,461 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:26,461 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:27,737 - DEBUG - Training batch loss: 7.339082112126474\n",
      "2025-11-02 13:38:27,737 - root - DEBUG - Training batch loss: 7.339082112126474\n",
      "2025-11-02 13:38:29,000 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:38:29,000 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:38:29,001 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:29,001 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:30,292 - DEBUG - Training batch loss: 7.155816921995188\n",
      "2025-11-02 13:38:30,292 - root - DEBUG - Training batch loss: 7.155816921995188\n",
      "2025-11-02 13:38:31,541 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:31,541 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:31,542 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:31,542 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:32,785 - DEBUG - Training batch loss: 7.28902853004427\n",
      "2025-11-02 13:38:32,785 - root - DEBUG - Training batch loss: 7.28902853004427\n",
      "2025-11-02 13:38:34,082 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:34,082 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:34,082 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:34,082 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:35,482 - DEBUG - Training batch loss: 8.32644770282846\n",
      "2025-11-02 13:38:35,482 - root - DEBUG - Training batch loss: 8.32644770282846\n",
      "2025-11-02 13:38:36,781 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:36,781 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:36,782 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:36,782 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:37,965 - DEBUG - Training batch loss: 7.55296949754291\n",
      "2025-11-02 13:38:37,965 - root - DEBUG - Training batch loss: 7.55296949754291\n",
      "2025-11-02 13:38:39,264 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:39,264 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:39,266 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:39,266 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:40,729 - DEBUG - Training batch loss: 7.536032146053912\n",
      "2025-11-02 13:38:40,729 - root - DEBUG - Training batch loss: 7.536032146053912\n",
      "2025-11-02 13:38:42,100 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:42,100 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:42,101 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:42,101 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:43,438 - DEBUG - Training batch loss: 6.955876816382197\n",
      "2025-11-02 13:38:43,438 - root - DEBUG - Training batch loss: 6.955876816382197\n",
      "2025-11-02 13:38:44,797 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:44,797 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:44,798 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:44,798 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:46,193 - DEBUG - Training batch loss: 6.849447755130845\n",
      "2025-11-02 13:38:46,193 - root - DEBUG - Training batch loss: 6.849447755130845\n",
      "2025-11-02 13:38:47,423 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:47,423 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:47,424 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:47,424 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:48,640 - DEBUG - Training batch loss: 6.6190086185815495\n",
      "2025-11-02 13:38:48,640 - root - DEBUG - Training batch loss: 6.6190086185815495\n",
      "2025-11-02 13:38:49,957 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:38:49,957 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:38:49,958 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:49,958 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:51,168 - DEBUG - Training batch loss: 6.979098717311896\n",
      "2025-11-02 13:38:51,168 - root - DEBUG - Training batch loss: 6.979098717311896\n",
      "2025-11-02 13:38:52,437 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:52,437 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:52,438 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:52,438 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:53,700 - DEBUG - Training batch loss: 6.545805078231876\n",
      "2025-11-02 13:38:53,700 - root - DEBUG - Training batch loss: 6.545805078231876\n",
      "2025-11-02 13:38:54,996 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:54,996 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:38:54,998 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:54,998 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:56,377 - DEBUG - Training batch loss: 7.395595310788167\n",
      "2025-11-02 13:38:56,377 - root - DEBUG - Training batch loss: 7.395595310788167\n",
      "2025-11-02 13:38:57,644 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:38:57,644 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:38:57,645 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:57,645 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:38:58,807 - DEBUG - Training batch loss: 7.034913889272064\n",
      "2025-11-02 13:38:58,807 - root - DEBUG - Training batch loss: 7.034913889272064\n",
      "2025-11-02 13:39:00,057 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:00,057 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:00,058 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:00,058 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:01,506 - DEBUG - Training batch loss: 8.030500107153502\n",
      "2025-11-02 13:39:01,506 - root - DEBUG - Training batch loss: 8.030500107153502\n",
      "2025-11-02 13:39:02,880 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:02,880 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:02,882 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:02,882 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:04,044 - DEBUG - Training batch loss: 5.890622558559849\n",
      "2025-11-02 13:39:04,044 - root - DEBUG - Training batch loss: 5.890622558559849\n",
      "2025-11-02 13:39:05,335 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:39:05,335 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:39:05,336 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:05,336 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:06,635 - DEBUG - Training batch loss: 6.748273767696779\n",
      "2025-11-02 13:39:06,635 - root - DEBUG - Training batch loss: 6.748273767696779\n",
      "2025-11-02 13:39:07,892 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:07,892 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:07,893 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:07,893 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:09,111 - DEBUG - Training batch loss: 6.809130357641257\n",
      "2025-11-02 13:39:09,111 - root - DEBUG - Training batch loss: 6.809130357641257\n",
      "2025-11-02 13:39:10,386 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:39:10,386 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:39:10,387 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:10,387 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:11,666 - DEBUG - Training batch loss: 7.689982418955713\n",
      "2025-11-02 13:39:11,666 - root - DEBUG - Training batch loss: 7.689982418955713\n",
      "2025-11-02 13:39:12,962 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:12,962 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:12,963 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:12,963 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:14,736 - DEBUG - Training batch loss: 7.427129824922743\n",
      "2025-11-02 13:39:14,736 - root - DEBUG - Training batch loss: 7.427129824922743\n",
      "2025-11-02 13:39:16,178 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:16,178 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:16,179 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:16,179 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:17,474 - DEBUG - Training batch loss: 7.322796517914684\n",
      "2025-11-02 13:39:17,474 - root - DEBUG - Training batch loss: 7.322796517914684\n",
      "2025-11-02 13:39:18,787 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:18,787 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:18,788 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:18,788 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:19,980 - DEBUG - Training batch loss: 7.129769123066735\n",
      "2025-11-02 13:39:19,980 - root - DEBUG - Training batch loss: 7.129769123066735\n",
      "2025-11-02 13:39:21,227 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:21,227 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:21,228 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:21,228 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:22,671 - DEBUG - Training batch loss: 7.984375553782076\n",
      "2025-11-02 13:39:22,671 - root - DEBUG - Training batch loss: 7.984375553782076\n",
      "2025-11-02 13:39:23,927 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:23,927 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:23,928 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:23,928 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:25,174 - DEBUG - Training batch loss: 6.740154805120207\n",
      "2025-11-02 13:39:25,174 - root - DEBUG - Training batch loss: 6.740154805120207\n",
      "2025-11-02 13:39:26,440 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:26,440 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:26,441 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:26,441 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:27,827 - DEBUG - Training batch loss: 7.392716964074947\n",
      "2025-11-02 13:39:27,827 - root - DEBUG - Training batch loss: 7.392716964074947\n",
      "2025-11-02 13:39:29,076 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:29,076 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:29,078 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:29,078 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:30,455 - DEBUG - Training batch loss: 8.04001555962367\n",
      "2025-11-02 13:39:30,455 - root - DEBUG - Training batch loss: 8.04001555962367\n",
      "2025-11-02 13:39:31,744 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:31,744 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:31,745 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:31,745 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:33,246 - DEBUG - Training batch loss: 6.11810931740829\n",
      "2025-11-02 13:39:33,246 - root - DEBUG - Training batch loss: 6.11810931740829\n",
      "2025-11-02 13:39:34,641 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:34,641 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:34,642 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:34,642 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:35,938 - DEBUG - Training batch loss: 7.337523934361632\n",
      "2025-11-02 13:39:35,938 - root - DEBUG - Training batch loss: 7.337523934361632\n",
      "2025-11-02 13:39:37,231 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:37,231 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:37,232 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:37,232 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:38,590 - DEBUG - Training batch loss: 7.078953346090348\n",
      "2025-11-02 13:39:38,590 - root - DEBUG - Training batch loss: 7.078953346090348\n",
      "2025-11-02 13:39:39,886 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:39,886 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:39,888 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:39,888 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:41,230 - DEBUG - Training batch loss: 7.592303361213196\n",
      "2025-11-02 13:39:41,230 - root - DEBUG - Training batch loss: 7.592303361213196\n",
      "2025-11-02 13:39:42,516 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:42,516 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:42,517 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:42,517 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:43,801 - DEBUG - Training batch loss: 7.910312762960348\n",
      "2025-11-02 13:39:43,801 - root - DEBUG - Training batch loss: 7.910312762960348\n",
      "2025-11-02 13:39:45,100 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:45,100 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:45,101 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:45,101 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:46,243 - DEBUG - Training batch loss: 6.020284715989561\n",
      "2025-11-02 13:39:46,243 - root - DEBUG - Training batch loss: 6.020284715989561\n",
      "2025-11-02 13:39:47,546 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:47,546 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:47,547 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:47,547 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:48,787 - DEBUG - Training batch loss: 7.053910225081543\n",
      "2025-11-02 13:39:48,787 - root - DEBUG - Training batch loss: 7.053910225081543\n",
      "2025-11-02 13:39:50,022 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:50,022 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:50,023 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:50,023 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:51,272 - DEBUG - Training batch loss: 7.4886980021534\n",
      "2025-11-02 13:39:51,272 - root - DEBUG - Training batch loss: 7.4886980021534\n",
      "2025-11-02 13:39:52,533 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:52,533 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:39:52,534 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:52,534 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:53,917 - DEBUG - Training batch loss: 7.553425211054512\n",
      "2025-11-02 13:39:53,917 - root - DEBUG - Training batch loss: 7.553425211054512\n",
      "2025-11-02 13:39:55,343 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:55,343 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:39:55,345 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:55,345 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:56,718 - DEBUG - Training batch loss: 6.281071456793887\n",
      "2025-11-02 13:39:56,718 - root - DEBUG - Training batch loss: 6.281071456793887\n",
      "2025-11-02 13:39:58,154 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:39:58,154 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:39:58,155 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:58,155 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:39:59,655 - DEBUG - Training batch loss: 7.176994887026432\n",
      "2025-11-02 13:39:59,655 - root - DEBUG - Training batch loss: 7.176994887026432\n",
      "2025-11-02 13:40:01,059 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:01,059 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:01,060 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:01,060 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:02,467 - DEBUG - Training batch loss: 6.832017721840301\n",
      "2025-11-02 13:40:02,467 - root - DEBUG - Training batch loss: 6.832017721840301\n",
      "2025-11-02 13:40:03,952 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:03,952 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:03,953 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:03,953 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:05,408 - DEBUG - Training batch loss: 7.513290844352448\n",
      "2025-11-02 13:40:05,408 - root - DEBUG - Training batch loss: 7.513290844352448\n",
      "2025-11-02 13:40:06,712 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:06,712 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:06,714 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:06,714 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:07,905 - DEBUG - Training batch loss: 7.244859751693443\n",
      "2025-11-02 13:40:07,905 - root - DEBUG - Training batch loss: 7.244859751693443\n",
      "2025-11-02 13:40:09,151 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:09,151 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:09,152 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:09,152 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:10,387 - DEBUG - Training batch loss: 6.979196281145747\n",
      "2025-11-02 13:40:10,387 - root - DEBUG - Training batch loss: 6.979196281145747\n",
      "2025-11-02 13:40:11,780 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:40:11,780 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:40:11,781 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:11,781 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:12,959 - DEBUG - Training batch loss: 6.676996115665623\n",
      "2025-11-02 13:40:12,959 - root - DEBUG - Training batch loss: 6.676996115665623\n",
      "2025-11-02 13:40:14,253 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:14,253 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:14,255 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:14,255 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:15,593 - DEBUG - Training batch loss: 6.944451514300978\n",
      "2025-11-02 13:40:15,593 - root - DEBUG - Training batch loss: 6.944451514300978\n",
      "2025-11-02 13:40:16,929 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:16,929 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:16,930 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:16,930 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:18,152 - DEBUG - Training batch loss: 7.820405259587095\n",
      "2025-11-02 13:40:18,152 - root - DEBUG - Training batch loss: 7.820405259587095\n",
      "2025-11-02 13:40:19,437 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:19,437 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:19,438 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:19,438 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:20,685 - DEBUG - Training batch loss: 6.607971007980529\n",
      "2025-11-02 13:40:20,685 - root - DEBUG - Training batch loss: 6.607971007980529\n",
      "2025-11-02 13:40:21,890 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:21,890 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:21,891 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:21,891 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:23,050 - DEBUG - Training batch loss: 6.357130906269414\n",
      "2025-11-02 13:40:23,050 - root - DEBUG - Training batch loss: 6.357130906269414\n",
      "2025-11-02 13:40:24,273 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:24,273 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:24,274 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:24,274 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:25,538 - DEBUG - Training batch loss: 6.293877512971346\n",
      "2025-11-02 13:40:25,538 - root - DEBUG - Training batch loss: 6.293877512971346\n",
      "2025-11-02 13:40:26,914 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:26,914 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:26,915 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:26,915 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:28,103 - DEBUG - Training batch loss: 7.105711875047818\n",
      "2025-11-02 13:40:28,103 - root - DEBUG - Training batch loss: 7.105711875047818\n",
      "2025-11-02 13:40:29,387 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:29,387 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:29,389 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:29,389 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:30,633 - DEBUG - Training batch loss: 7.083601826569118\n",
      "2025-11-02 13:40:30,633 - root - DEBUG - Training batch loss: 7.083601826569118\n",
      "2025-11-02 13:40:31,880 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:31,880 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:31,881 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:31,881 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:33,081 - DEBUG - Training batch loss: 7.128734815105654\n",
      "2025-11-02 13:40:33,081 - root - DEBUG - Training batch loss: 7.128734815105654\n",
      "2025-11-02 13:40:34,379 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:34,379 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:34,380 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:34,380 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:35,639 - DEBUG - Training batch loss: 7.903675016072274\n",
      "2025-11-02 13:40:35,639 - root - DEBUG - Training batch loss: 7.903675016072274\n",
      "2025-11-02 13:40:36,866 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:36,866 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:36,868 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:36,868 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:38,026 - DEBUG - Training batch loss: 7.321800886646745\n",
      "2025-11-02 13:40:38,026 - root - DEBUG - Training batch loss: 7.321800886646745\n",
      "2025-11-02 13:40:39,360 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:39,360 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:39,361 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:39,361 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:40,814 - DEBUG - Training batch loss: 6.208660854646126\n",
      "2025-11-02 13:40:40,814 - root - DEBUG - Training batch loss: 6.208660854646126\n",
      "2025-11-02 13:40:42,207 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:42,207 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:42,209 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:42,209 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:43,363 - DEBUG - Training batch loss: 7.16468268456525\n",
      "2025-11-02 13:40:43,363 - root - DEBUG - Training batch loss: 7.16468268456525\n",
      "2025-11-02 13:40:44,616 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:44,616 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:44,617 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:44,617 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:45,876 - DEBUG - Training batch loss: 7.375452747491457\n",
      "2025-11-02 13:40:45,876 - root - DEBUG - Training batch loss: 7.375452747491457\n",
      "2025-11-02 13:40:47,117 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:47,117 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:40:47,118 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:47,118 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:48,298 - DEBUG - Training batch loss: 7.954310881168081\n",
      "2025-11-02 13:40:48,298 - root - DEBUG - Training batch loss: 7.954310881168081\n",
      "2025-11-02 13:40:49,578 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:49,578 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:49,579 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:49,579 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:50,871 - DEBUG - Training batch loss: 7.109718190931128\n",
      "2025-11-02 13:40:50,871 - root - DEBUG - Training batch loss: 7.109718190931128\n",
      "2025-11-02 13:40:52,238 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:52,238 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:52,239 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:52,239 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:53,643 - DEBUG - Training batch loss: 6.516390428439954\n",
      "2025-11-02 13:40:53,643 - root - DEBUG - Training batch loss: 6.516390428439954\n",
      "2025-11-02 13:40:55,006 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:55,006 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:55,007 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:55,007 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:56,272 - DEBUG - Training batch loss: 7.305311270335334\n",
      "2025-11-02 13:40:56,272 - root - DEBUG - Training batch loss: 7.305311270335334\n",
      "2025-11-02 13:40:57,497 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:57,497 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:40:57,498 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:57,498 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:40:58,952 - DEBUG - Training batch loss: 7.31524481848039\n",
      "2025-11-02 13:40:58,952 - root - DEBUG - Training batch loss: 7.31524481848039\n",
      "2025-11-02 13:41:00,339 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:00,339 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:00,341 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:00,341 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:01,694 - DEBUG - Training batch loss: 7.6023983187197075\n",
      "2025-11-02 13:41:01,694 - root - DEBUG - Training batch loss: 7.6023983187197075\n",
      "2025-11-02 13:41:03,012 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:03,012 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:03,014 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:03,014 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:04,780 - DEBUG - Training batch loss: 7.735258514215396\n",
      "2025-11-02 13:41:04,780 - root - DEBUG - Training batch loss: 7.735258514215396\n",
      "2025-11-02 13:41:06,138 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:06,138 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:06,139 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:06,139 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:07,362 - DEBUG - Training batch loss: 7.156061381599473\n",
      "2025-11-02 13:41:07,362 - root - DEBUG - Training batch loss: 7.156061381599473\n",
      "2025-11-02 13:41:08,591 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:08,591 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:08,592 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:08,592 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:09,812 - DEBUG - Training batch loss: 6.661189894758673\n",
      "2025-11-02 13:41:09,812 - root - DEBUG - Training batch loss: 6.661189894758673\n",
      "2025-11-02 13:41:11,078 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:11,078 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:11,079 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:11,079 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:12,401 - DEBUG - Training batch loss: 7.351898706270703\n",
      "2025-11-02 13:41:12,401 - root - DEBUG - Training batch loss: 7.351898706270703\n",
      "2025-11-02 13:41:13,667 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:13,667 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:13,668 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:13,668 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:14,830 - DEBUG - Training batch loss: 7.378737011382185\n",
      "2025-11-02 13:41:14,830 - root - DEBUG - Training batch loss: 7.378737011382185\n",
      "2025-11-02 13:41:16,195 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:16,195 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:16,196 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:16,196 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:17,443 - DEBUG - Training batch loss: 6.5747681541402025\n",
      "2025-11-02 13:41:17,443 - root - DEBUG - Training batch loss: 6.5747681541402025\n",
      "2025-11-02 13:41:18,781 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:41:18,781 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:41:18,782 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:18,782 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:19,997 - DEBUG - Training batch loss: 7.330594711170165\n",
      "2025-11-02 13:41:19,997 - root - DEBUG - Training batch loss: 7.330594711170165\n",
      "2025-11-02 13:41:21,315 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:21,315 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:21,316 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:21,316 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:22,586 - DEBUG - Training batch loss: 7.245271823967214\n",
      "2025-11-02 13:41:22,586 - root - DEBUG - Training batch loss: 7.245271823967214\n",
      "2025-11-02 13:41:23,924 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:23,924 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:23,925 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:23,925 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:25,251 - DEBUG - Training batch loss: 7.249374592289773\n",
      "2025-11-02 13:41:25,251 - root - DEBUG - Training batch loss: 7.249374592289773\n",
      "2025-11-02 13:41:26,580 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:26,580 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:26,581 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:26,581 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:27,820 - DEBUG - Training batch loss: 6.254586029231522\n",
      "2025-11-02 13:41:27,820 - root - DEBUG - Training batch loss: 6.254586029231522\n",
      "2025-11-02 13:41:29,044 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:29,044 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:29,045 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:29,045 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:30,369 - DEBUG - Training batch loss: 7.073701404748933\n",
      "2025-11-02 13:41:30,369 - root - DEBUG - Training batch loss: 7.073701404748933\n",
      "2025-11-02 13:41:31,714 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:31,714 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:31,715 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:31,715 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:33,027 - DEBUG - Training batch loss: 7.710667514080759\n",
      "2025-11-02 13:41:33,027 - root - DEBUG - Training batch loss: 7.710667514080759\n",
      "2025-11-02 13:41:34,390 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:34,390 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:34,392 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:34,392 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:35,904 - DEBUG - Training batch loss: 7.591003037361292\n",
      "2025-11-02 13:41:35,904 - root - DEBUG - Training batch loss: 7.591003037361292\n",
      "2025-11-02 13:41:37,176 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:37,176 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:37,177 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:37,177 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:38,377 - DEBUG - Training batch loss: 7.0807804268936625\n",
      "2025-11-02 13:41:38,377 - root - DEBUG - Training batch loss: 7.0807804268936625\n",
      "2025-11-02 13:41:39,767 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:39,767 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:39,768 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:39,768 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:41,025 - DEBUG - Training batch loss: 7.1773055877528\n",
      "2025-11-02 13:41:41,025 - root - DEBUG - Training batch loss: 7.1773055877528\n",
      "2025-11-02 13:41:42,276 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:42,276 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:42,277 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:42,277 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:43,671 - DEBUG - Training batch loss: 7.676514270431481\n",
      "2025-11-02 13:41:43,671 - root - DEBUG - Training batch loss: 7.676514270431481\n",
      "2025-11-02 13:41:44,986 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:44,986 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:44,987 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:44,987 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:46,284 - DEBUG - Training batch loss: 6.970453575156877\n",
      "2025-11-02 13:41:46,284 - root - DEBUG - Training batch loss: 6.970453575156877\n",
      "2025-11-02 13:41:47,839 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:47,839 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:47,839 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:47,839 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:49,272 - DEBUG - Training batch loss: 7.14230281547279\n",
      "2025-11-02 13:41:49,272 - root - DEBUG - Training batch loss: 7.14230281547279\n",
      "2025-11-02 13:41:50,567 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:50,567 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:50,568 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:50,568 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:52,141 - DEBUG - Training batch loss: 7.395827773130105\n",
      "2025-11-02 13:41:52,141 - root - DEBUG - Training batch loss: 7.395827773130105\n",
      "2025-11-02 13:41:53,538 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:53,538 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:41:53,539 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:53,539 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:54,964 - DEBUG - Training batch loss: 6.894987456832532\n",
      "2025-11-02 13:41:54,964 - root - DEBUG - Training batch loss: 6.894987456832532\n",
      "2025-11-02 13:41:56,296 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:56,296 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:56,297 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:56,297 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:57,662 - DEBUG - Training batch loss: 7.987817391999899\n",
      "2025-11-02 13:41:57,662 - root - DEBUG - Training batch loss: 7.987817391999899\n",
      "2025-11-02 13:41:58,933 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:58,933 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:41:58,935 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:41:58,935 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:00,433 - DEBUG - Training batch loss: 8.096824171140142\n",
      "2025-11-02 13:42:00,433 - root - DEBUG - Training batch loss: 8.096824171140142\n",
      "2025-11-02 13:42:01,838 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:01,838 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:01,839 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:01,839 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:03,045 - DEBUG - Training batch loss: 6.22940522841411\n",
      "2025-11-02 13:42:03,045 - root - DEBUG - Training batch loss: 6.22940522841411\n",
      "2025-11-02 13:42:04,360 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:04,360 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:04,361 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:04,361 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:05,613 - DEBUG - Training batch loss: 6.876535472286317\n",
      "2025-11-02 13:42:05,613 - root - DEBUG - Training batch loss: 6.876535472286317\n",
      "2025-11-02 13:42:06,970 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:06,970 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:06,971 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:06,971 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:08,421 - DEBUG - Training batch loss: 6.899770811359971\n",
      "2025-11-02 13:42:08,421 - root - DEBUG - Training batch loss: 6.899770811359971\n",
      "2025-11-02 13:42:09,769 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:09,769 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:09,771 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:09,771 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:11,144 - DEBUG - Training batch loss: 7.133614167092707\n",
      "2025-11-02 13:42:11,144 - root - DEBUG - Training batch loss: 7.133614167092707\n",
      "2025-11-02 13:42:12,444 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:12,444 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:12,445 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:12,445 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:13,791 - DEBUG - Training batch loss: 6.586511574245341\n",
      "2025-11-02 13:42:13,791 - root - DEBUG - Training batch loss: 6.586511574245341\n",
      "2025-11-02 13:42:15,217 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:15,217 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:15,218 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:15,218 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:16,562 - DEBUG - Training batch loss: 7.55520076940198\n",
      "2025-11-02 13:42:16,562 - root - DEBUG - Training batch loss: 7.55520076940198\n",
      "2025-11-02 13:42:17,851 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:17,851 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:17,852 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:17,852 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:19,090 - DEBUG - Training batch loss: 7.689046817227344\n",
      "2025-11-02 13:42:19,090 - root - DEBUG - Training batch loss: 7.689046817227344\n",
      "2025-11-02 13:42:20,410 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:20,410 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:20,411 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:20,411 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:21,734 - DEBUG - Training batch loss: 7.663870143338903\n",
      "2025-11-02 13:42:21,734 - root - DEBUG - Training batch loss: 7.663870143338903\n",
      "2025-11-02 13:42:23,001 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:23,001 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:23,003 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:23,003 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:24,207 - DEBUG - Training batch loss: 7.143943741831712\n",
      "2025-11-02 13:42:24,207 - root - DEBUG - Training batch loss: 7.143943741831712\n",
      "2025-11-02 13:42:25,473 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:25,473 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:25,474 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:25,474 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:26,691 - DEBUG - Training batch loss: 7.3859233694952415\n",
      "2025-11-02 13:42:26,691 - root - DEBUG - Training batch loss: 7.3859233694952415\n",
      "2025-11-02 13:42:27,916 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:27,916 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:27,917 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:27,917 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:29,087 - DEBUG - Training batch loss: 7.202413692504379\n",
      "2025-11-02 13:42:29,087 - root - DEBUG - Training batch loss: 7.202413692504379\n",
      "2025-11-02 13:42:30,338 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:30,338 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:30,339 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:30,339 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:31,625 - DEBUG - Training batch loss: 7.011245698828562\n",
      "2025-11-02 13:42:31,625 - root - DEBUG - Training batch loss: 7.011245698828562\n",
      "2025-11-02 13:42:32,859 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:32,859 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:32,859 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:32,859 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:34,058 - DEBUG - Training batch loss: 7.129903847312848\n",
      "2025-11-02 13:42:34,058 - root - DEBUG - Training batch loss: 7.129903847312848\n",
      "2025-11-02 13:42:35,356 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:35,356 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:35,357 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:35,357 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:36,620 - DEBUG - Training batch loss: 6.933954215450635\n",
      "2025-11-02 13:42:36,620 - root - DEBUG - Training batch loss: 6.933954215450635\n",
      "2025-11-02 13:42:37,850 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:37,850 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:37,851 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:37,851 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:38,990 - DEBUG - Training batch loss: 7.72677201202927\n",
      "2025-11-02 13:42:38,990 - root - DEBUG - Training batch loss: 7.72677201202927\n",
      "2025-11-02 13:42:40,242 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:40,242 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:40,243 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:40,243 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:41,481 - DEBUG - Training batch loss: 7.354317278983236\n",
      "2025-11-02 13:42:41,481 - root - DEBUG - Training batch loss: 7.354317278983236\n",
      "2025-11-02 13:42:42,698 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:42,698 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:42,699 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:42,699 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:43,870 - DEBUG - Training batch loss: 6.935249590336353\n",
      "2025-11-02 13:42:43,870 - root - DEBUG - Training batch loss: 6.935249590336353\n",
      "2025-11-02 13:42:45,191 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:45,191 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:45,193 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:45,193 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:46,546 - DEBUG - Training batch loss: 7.303022666217068\n",
      "2025-11-02 13:42:46,546 - root - DEBUG - Training batch loss: 7.303022666217068\n",
      "2025-11-02 13:42:47,793 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:47,793 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:47,795 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:47,795 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:49,089 - DEBUG - Training batch loss: 6.683299189171605\n",
      "2025-11-02 13:42:49,089 - root - DEBUG - Training batch loss: 6.683299189171605\n",
      "2025-11-02 13:42:50,392 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:50,392 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:50,394 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:50,394 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:51,713 - DEBUG - Training batch loss: 6.415524661463693\n",
      "2025-11-02 13:42:51,713 - root - DEBUG - Training batch loss: 6.415524661463693\n",
      "2025-11-02 13:42:52,947 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:52,947 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:42:52,948 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:52,948 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:54,327 - DEBUG - Training batch loss: 7.281173266998214\n",
      "2025-11-02 13:42:54,327 - root - DEBUG - Training batch loss: 7.281173266998214\n",
      "2025-11-02 13:42:55,618 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:55,618 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:42:55,619 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:55,619 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:56,811 - DEBUG - Training batch loss: 6.877238581032305\n",
      "2025-11-02 13:42:56,811 - root - DEBUG - Training batch loss: 6.877238581032305\n",
      "2025-11-02 13:42:58,178 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:42:58,178 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:42:58,179 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:58,179 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:42:59,701 - DEBUG - Training batch loss: 5.994350478825191\n",
      "2025-11-02 13:42:59,701 - root - DEBUG - Training batch loss: 5.994350478825191\n",
      "2025-11-02 13:43:00,993 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:00,993 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:00,994 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:00,994 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:02,679 - DEBUG - Training batch loss: 7.315641659782859\n",
      "2025-11-02 13:43:02,679 - root - DEBUG - Training batch loss: 7.315641659782859\n",
      "2025-11-02 13:43:04,010 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:43:04,010 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:43:04,011 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:04,011 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:05,211 - DEBUG - Training batch loss: 7.355414261857204\n",
      "2025-11-02 13:43:05,211 - root - DEBUG - Training batch loss: 7.355414261857204\n",
      "2025-11-02 13:43:06,612 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:06,612 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:06,614 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:06,614 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:08,137 - DEBUG - Training batch loss: 6.207514378097201\n",
      "2025-11-02 13:43:08,137 - root - DEBUG - Training batch loss: 6.207514378097201\n",
      "2025-11-02 13:43:09,494 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:09,494 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:09,495 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:09,495 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:10,814 - DEBUG - Training batch loss: 6.65743972584941\n",
      "2025-11-02 13:43:10,814 - root - DEBUG - Training batch loss: 6.65743972584941\n",
      "2025-11-02 13:43:12,158 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:12,158 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:12,159 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:12,159 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:13,438 - DEBUG - Training batch loss: 7.263773788433686\n",
      "2025-11-02 13:43:13,438 - root - DEBUG - Training batch loss: 7.263773788433686\n",
      "2025-11-02 13:43:14,684 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:14,684 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:14,686 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:14,686 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:15,867 - DEBUG - Training batch loss: 7.2767014693269525\n",
      "2025-11-02 13:43:15,867 - root - DEBUG - Training batch loss: 7.2767014693269525\n",
      "2025-11-02 13:43:17,146 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:17,146 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:17,147 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:17,147 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:18,474 - DEBUG - Training batch loss: 7.3003344479241585\n",
      "2025-11-02 13:43:18,474 - root - DEBUG - Training batch loss: 7.3003344479241585\n",
      "2025-11-02 13:43:19,728 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:19,728 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:19,729 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:19,729 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:20,882 - DEBUG - Training batch loss: 7.716616670014877\n",
      "2025-11-02 13:43:20,882 - root - DEBUG - Training batch loss: 7.716616670014877\n",
      "2025-11-02 13:43:22,139 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:22,139 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:22,140 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:22,140 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:23,563 - DEBUG - Training batch loss: 7.42403229756897\n",
      "2025-11-02 13:43:23,563 - root - DEBUG - Training batch loss: 7.42403229756897\n",
      "2025-11-02 13:43:24,923 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:24,923 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:24,925 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:24,925 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:26,174 - DEBUG - Training batch loss: 7.020528709424284\n",
      "2025-11-02 13:43:26,174 - root - DEBUG - Training batch loss: 7.020528709424284\n",
      "2025-11-02 13:43:27,485 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:27,485 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:43:27,486 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:27,486 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:43:28,781 - DEBUG - Training batch loss: 7.55218370573319\n",
      "2025-11-02 13:43:28,781 - root - DEBUG - Training batch loss: 7.55218370573319\n",
      "2025-11-02 13:43:30,102 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:30,102 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:43:30,103 - INFO - Epoch 2/3, Train Loss: 7.1509,Train Accuracy: 0.0127\n",
      "2025-11-02 13:43:30,103 - root - INFO - Epoch 2/3, Train Loss: 7.1509,Train Accuracy: 0.0127\n",
      "2025-11-02 13:43:30,148 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:30,148 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:31,576 - DEBUG - Validation batch loss: 15.372243776638799\n",
      "2025-11-02 13:43:31,576 - root - DEBUG - Validation batch loss: 15.372243776638799\n",
      "2025-11-02 13:43:31,577 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:31,577 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:31,578 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:31,578 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:32,753 - DEBUG - Validation batch loss: 26.599287358685217\n",
      "2025-11-02 13:43:32,753 - root - DEBUG - Validation batch loss: 26.599287358685217\n",
      "2025-11-02 13:43:32,754 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:32,754 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:32,755 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:32,755 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:33,943 - DEBUG - Validation batch loss: 28.149134573200072\n",
      "2025-11-02 13:43:33,943 - root - DEBUG - Validation batch loss: 28.149134573200072\n",
      "2025-11-02 13:43:33,946 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:33,946 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:33,947 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:33,947 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:35,209 - DEBUG - Validation batch loss: 28.149134573200072\n",
      "2025-11-02 13:43:35,209 - root - DEBUG - Validation batch loss: 28.149134573200072\n",
      "2025-11-02 13:43:35,210 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:35,210 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:35,211 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:35,211 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:36,535 - DEBUG - Validation batch loss: 27.70201525957237\n",
      "2025-11-02 13:43:36,535 - root - DEBUG - Validation batch loss: 27.70201525957237\n",
      "2025-11-02 13:43:36,536 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:36,536 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:36,537 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:36,537 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:37,735 - DEBUG - Validation batch loss: 14.29614016787012\n",
      "2025-11-02 13:43:37,735 - root - DEBUG - Validation batch loss: 14.29614016787012\n",
      "2025-11-02 13:43:37,738 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:37,738 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:37,739 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:37,739 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:39,404 - DEBUG - Validation batch loss: 9.902263134765278\n",
      "2025-11-02 13:43:39,404 - root - DEBUG - Validation batch loss: 9.902263134765278\n",
      "2025-11-02 13:43:39,407 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:39,407 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:39,410 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:39,410 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:40,709 - DEBUG - Validation batch loss: 9.752603083558226\n",
      "2025-11-02 13:43:40,709 - root - DEBUG - Validation batch loss: 9.752603083558226\n",
      "2025-11-02 13:43:40,712 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:40,712 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:40,713 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:40,713 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:42,074 - DEBUG - Validation batch loss: 7.901241836195521\n",
      "2025-11-02 13:43:42,074 - root - DEBUG - Validation batch loss: 7.901241836195521\n",
      "2025-11-02 13:43:42,076 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:42,076 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:42,076 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:42,076 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:43,503 - DEBUG - Validation batch loss: 21.77806566177746\n",
      "2025-11-02 13:43:43,503 - root - DEBUG - Validation batch loss: 21.77806566177746\n",
      "2025-11-02 13:43:43,508 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:43,508 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:43,510 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:43,510 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:44,836 - DEBUG - Validation batch loss: 12.168738725098535\n",
      "2025-11-02 13:43:44,836 - root - DEBUG - Validation batch loss: 12.168738725098535\n",
      "2025-11-02 13:43:44,837 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:44,837 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:44,838 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:44,838 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:46,280 - DEBUG - Validation batch loss: 16.76894739968485\n",
      "2025-11-02 13:43:46,280 - root - DEBUG - Validation batch loss: 16.76894739968485\n",
      "2025-11-02 13:43:46,281 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:46,281 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:46,285 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:46,285 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:47,774 - DEBUG - Validation batch loss: 32.85561719252199\n",
      "2025-11-02 13:43:47,774 - root - DEBUG - Validation batch loss: 32.85561719252199\n",
      "2025-11-02 13:43:47,777 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:47,777 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:47,780 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:47,780 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:49,237 - DEBUG - Validation batch loss: 13.592123260007615\n",
      "2025-11-02 13:43:49,237 - root - DEBUG - Validation batch loss: 13.592123260007615\n",
      "2025-11-02 13:43:49,240 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:49,240 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:49,242 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:49,242 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:50,582 - DEBUG - Validation batch loss: 19.688856906942615\n",
      "2025-11-02 13:43:50,582 - root - DEBUG - Validation batch loss: 19.688856906942615\n",
      "2025-11-02 13:43:50,583 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:50,583 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:50,584 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:50,584 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:51,836 - DEBUG - Validation batch loss: 35.31509333204837\n",
      "2025-11-02 13:43:51,836 - root - DEBUG - Validation batch loss: 35.31509333204837\n",
      "2025-11-02 13:43:51,837 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:51,837 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:51,837 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:51,837 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:53,056 - DEBUG - Validation batch loss: 14.193504145048422\n",
      "2025-11-02 13:43:53,056 - root - DEBUG - Validation batch loss: 14.193504145048422\n",
      "2025-11-02 13:43:53,057 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:53,057 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:53,058 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:53,058 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:54,239 - DEBUG - Validation batch loss: 13.102777932461581\n",
      "2025-11-02 13:43:54,239 - root - DEBUG - Validation batch loss: 13.102777932461581\n",
      "2025-11-02 13:43:54,241 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:54,241 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:54,244 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:54,244 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:55,466 - DEBUG - Validation batch loss: 11.33059452533739\n",
      "2025-11-02 13:43:55,466 - root - DEBUG - Validation batch loss: 11.33059452533739\n",
      "2025-11-02 13:43:55,468 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:55,468 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:55,468 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:55,468 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:56,781 - DEBUG - Validation batch loss: 15.12291620154981\n",
      "2025-11-02 13:43:56,781 - root - DEBUG - Validation batch loss: 15.12291620154981\n",
      "2025-11-02 13:43:56,782 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:56,782 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:56,783 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:56,783 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:58,018 - DEBUG - Validation batch loss: 20.245848141014417\n",
      "2025-11-02 13:43:58,018 - root - DEBUG - Validation batch loss: 20.245848141014417\n",
      "2025-11-02 13:43:58,020 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:58,020 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:58,021 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:58,021 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:59,299 - DEBUG - Validation batch loss: 21.815103180888926\n",
      "2025-11-02 13:43:59,299 - root - DEBUG - Validation batch loss: 21.815103180888926\n",
      "2025-11-02 13:43:59,302 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:59,302 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:43:59,303 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:43:59,303 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:00,504 - DEBUG - Validation batch loss: 39.17501417722292\n",
      "2025-11-02 13:44:00,504 - root - DEBUG - Validation batch loss: 39.17501417722292\n",
      "2025-11-02 13:44:00,505 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:00,505 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:00,506 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:00,506 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:01,855 - DEBUG - Validation batch loss: 20.577952422707597\n",
      "2025-11-02 13:44:01,855 - root - DEBUG - Validation batch loss: 20.577952422707597\n",
      "2025-11-02 13:44:01,856 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:01,856 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:01,856 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:01,856 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:03,105 - DEBUG - Validation batch loss: 22.75569267198153\n",
      "2025-11-02 13:44:03,105 - root - DEBUG - Validation batch loss: 22.75569267198153\n",
      "2025-11-02 13:44:03,107 - DEBUG - Validation batch accuracy: 0.03125\n",
      "2025-11-02 13:44:03,107 - root - DEBUG - Validation batch accuracy: 0.03125\n",
      "2025-11-02 13:44:03,107 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:03,107 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:04,280 - DEBUG - Validation batch loss: 17.163793355703575\n",
      "2025-11-02 13:44:04,280 - root - DEBUG - Validation batch loss: 17.163793355703575\n",
      "2025-11-02 13:44:04,283 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:04,283 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:04,286 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:04,286 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:44:05,516 - DEBUG - Validation batch loss: 46.9924865003667\n",
      "2025-11-02 13:44:05,516 - root - DEBUG - Validation batch loss: 46.9924865003667\n",
      "2025-11-02 13:44:05,518 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:05,518 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:44:05,521 - INFO - Epoch 2/3,Val Loss: 20.8321, Val Accuracy: 0.0012\n",
      "2025-11-02 13:44:05,521 - root - INFO - Epoch 2/3,Val Loss: 20.8321, Val Accuracy: 0.0012\n",
      "2025-11-02 13:44:05,785 - DEBUG - Checkpoint saved to checkpoints/model_epoch_2.npz\n",
      "2025-11-02 13:44:05,785 - root - DEBUG - Checkpoint saved to checkpoints/model_epoch_2.npz\n",
      "2025-11-02 13:44:05,786 - DEBUG - Starting epoch 3/3.\n",
      "2025-11-02 13:44:05,786 - root - DEBUG - Starting epoch 3/3.\n",
      "2025-11-02 13:44:05,854 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:05,854 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:07,105 - DEBUG - Training batch loss: 6.23425993422249\n",
      "2025-11-02 13:44:07,105 - root - DEBUG - Training batch loss: 6.23425993422249\n",
      "2025-11-02 13:44:08,381 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:08,381 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:08,382 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:08,382 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:09,624 - DEBUG - Training batch loss: 6.532309782111611\n",
      "2025-11-02 13:44:09,624 - root - DEBUG - Training batch loss: 6.532309782111611\n",
      "2025-11-02 13:44:10,991 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:10,991 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:10,992 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:10,992 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:12,259 - DEBUG - Training batch loss: 7.5142071188799875\n",
      "2025-11-02 13:44:12,259 - root - DEBUG - Training batch loss: 7.5142071188799875\n",
      "2025-11-02 13:44:13,616 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:13,616 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:13,617 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:13,617 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:14,840 - DEBUG - Training batch loss: 7.616602181314536\n",
      "2025-11-02 13:44:14,840 - root - DEBUG - Training batch loss: 7.616602181314536\n",
      "2025-11-02 13:44:16,190 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:16,190 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:16,191 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:16,191 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:17,412 - DEBUG - Training batch loss: 7.38993432200521\n",
      "2025-11-02 13:44:17,412 - root - DEBUG - Training batch loss: 7.38993432200521\n",
      "2025-11-02 13:44:18,818 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:18,818 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:18,820 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:18,820 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:20,259 - DEBUG - Training batch loss: 8.002545951523045\n",
      "2025-11-02 13:44:20,259 - root - DEBUG - Training batch loss: 8.002545951523045\n",
      "2025-11-02 13:44:21,732 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:21,732 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:21,733 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:21,733 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:23,219 - DEBUG - Training batch loss: 7.15085977731577\n",
      "2025-11-02 13:44:23,219 - root - DEBUG - Training batch loss: 7.15085977731577\n",
      "2025-11-02 13:44:24,726 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:24,726 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:24,727 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:24,727 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:26,481 - DEBUG - Training batch loss: 7.415002347958138\n",
      "2025-11-02 13:44:26,481 - root - DEBUG - Training batch loss: 7.415002347958138\n",
      "2025-11-02 13:44:27,929 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:27,929 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:27,929 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:27,929 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:29,303 - DEBUG - Training batch loss: 6.551983050412217\n",
      "2025-11-02 13:44:29,303 - root - DEBUG - Training batch loss: 6.551983050412217\n",
      "2025-11-02 13:44:30,599 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:30,599 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:30,600 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:30,600 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:32,289 - DEBUG - Training batch loss: 7.2964704574861505\n",
      "2025-11-02 13:44:32,289 - root - DEBUG - Training batch loss: 7.2964704574861505\n",
      "2025-11-02 13:44:33,650 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:33,650 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:33,652 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:33,652 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:34,909 - DEBUG - Training batch loss: 7.407633032269738\n",
      "2025-11-02 13:44:34,909 - root - DEBUG - Training batch loss: 7.407633032269738\n",
      "2025-11-02 13:44:36,170 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:36,170 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:36,171 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:36,171 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:37,354 - DEBUG - Training batch loss: 6.557787475152684\n",
      "2025-11-02 13:44:37,354 - root - DEBUG - Training batch loss: 6.557787475152684\n",
      "2025-11-02 13:44:38,733 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:38,733 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:38,735 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:38,735 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:39,917 - DEBUG - Training batch loss: 7.248322353359477\n",
      "2025-11-02 13:44:39,917 - root - DEBUG - Training batch loss: 7.248322353359477\n",
      "2025-11-02 13:44:41,173 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:41,173 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:41,174 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:41,174 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:42,328 - DEBUG - Training batch loss: 7.255375432238479\n",
      "2025-11-02 13:44:42,328 - root - DEBUG - Training batch loss: 7.255375432238479\n",
      "2025-11-02 13:44:43,698 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:43,698 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:43,699 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:43,699 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:44,934 - DEBUG - Training batch loss: 7.166505123502969\n",
      "2025-11-02 13:44:44,934 - root - DEBUG - Training batch loss: 7.166505123502969\n",
      "2025-11-02 13:44:46,196 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:44:46,196 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:44:46,197 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:46,197 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:47,453 - DEBUG - Training batch loss: 6.738108600104457\n",
      "2025-11-02 13:44:47,453 - root - DEBUG - Training batch loss: 6.738108600104457\n",
      "2025-11-02 13:44:48,879 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:48,879 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:48,880 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:48,880 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:50,253 - DEBUG - Training batch loss: 6.642157657868204\n",
      "2025-11-02 13:44:50,253 - root - DEBUG - Training batch loss: 6.642157657868204\n",
      "2025-11-02 13:44:51,637 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:51,637 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:51,639 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:51,639 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:52,848 - DEBUG - Training batch loss: 7.28215210000086\n",
      "2025-11-02 13:44:52,848 - root - DEBUG - Training batch loss: 7.28215210000086\n",
      "2025-11-02 13:44:54,190 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:54,190 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:54,191 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:54,191 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:55,592 - DEBUG - Training batch loss: 7.020696679165503\n",
      "2025-11-02 13:44:55,592 - root - DEBUG - Training batch loss: 7.020696679165503\n",
      "2025-11-02 13:44:56,985 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:44:56,985 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:44:56,987 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:56,987 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:58,379 - DEBUG - Training batch loss: 7.930545328796024\n",
      "2025-11-02 13:44:58,379 - root - DEBUG - Training batch loss: 7.930545328796024\n",
      "2025-11-02 13:44:59,753 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:59,753 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:44:59,754 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:44:59,754 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:01,111 - DEBUG - Training batch loss: 7.2270696846290985\n",
      "2025-11-02 13:45:01,111 - root - DEBUG - Training batch loss: 7.2270696846290985\n",
      "2025-11-02 13:45:02,433 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:02,433 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:02,434 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:02,434 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:03,640 - DEBUG - Training batch loss: 6.9646011356766895\n",
      "2025-11-02 13:45:03,640 - root - DEBUG - Training batch loss: 6.9646011356766895\n",
      "2025-11-02 13:45:04,898 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:04,898 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:04,900 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:04,900 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:06,167 - DEBUG - Training batch loss: 6.592895344653807\n",
      "2025-11-02 13:45:06,167 - root - DEBUG - Training batch loss: 6.592895344653807\n",
      "2025-11-02 13:45:07,546 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:45:07,546 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:45:07,547 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:07,547 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:08,701 - DEBUG - Training batch loss: 7.187568363754826\n",
      "2025-11-02 13:45:08,701 - root - DEBUG - Training batch loss: 7.187568363754826\n",
      "2025-11-02 13:45:09,970 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:09,970 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:09,971 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:09,971 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:11,218 - DEBUG - Training batch loss: 7.058351809374034\n",
      "2025-11-02 13:45:11,218 - root - DEBUG - Training batch loss: 7.058351809374034\n",
      "2025-11-02 13:45:12,439 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:12,439 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:12,440 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:12,440 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:13,650 - DEBUG - Training batch loss: 7.768803977733418\n",
      "2025-11-02 13:45:13,650 - root - DEBUG - Training batch loss: 7.768803977733418\n",
      "2025-11-02 13:45:14,916 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:14,916 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:14,917 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:14,917 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:16,148 - DEBUG - Training batch loss: 6.75735877673021\n",
      "2025-11-02 13:45:16,148 - root - DEBUG - Training batch loss: 6.75735877673021\n",
      "2025-11-02 13:45:17,400 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:17,400 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:17,401 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:17,401 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:18,586 - DEBUG - Training batch loss: 6.797913114997829\n",
      "2025-11-02 13:45:18,586 - root - DEBUG - Training batch loss: 6.797913114997829\n",
      "2025-11-02 13:45:19,844 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:19,844 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:19,845 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:19,845 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:21,052 - DEBUG - Training batch loss: 8.220076911541074\n",
      "2025-11-02 13:45:21,052 - root - DEBUG - Training batch loss: 8.220076911541074\n",
      "2025-11-02 13:45:22,255 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:22,255 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:22,256 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:22,256 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:23,414 - DEBUG - Training batch loss: 6.9872448467769\n",
      "2025-11-02 13:45:23,414 - root - DEBUG - Training batch loss: 6.9872448467769\n",
      "2025-11-02 13:45:24,768 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:24,768 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:24,769 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:24,769 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:26,135 - DEBUG - Training batch loss: 8.053059293953147\n",
      "2025-11-02 13:45:26,135 - root - DEBUG - Training batch loss: 8.053059293953147\n",
      "2025-11-02 13:45:27,455 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:27,455 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:27,456 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:27,456 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:28,632 - DEBUG - Training batch loss: 7.756024325366202\n",
      "2025-11-02 13:45:28,632 - root - DEBUG - Training batch loss: 7.756024325366202\n",
      "2025-11-02 13:45:29,940 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:29,940 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:29,941 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:29,941 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:31,167 - DEBUG - Training batch loss: 6.9391622610794865\n",
      "2025-11-02 13:45:31,167 - root - DEBUG - Training batch loss: 6.9391622610794865\n",
      "2025-11-02 13:45:32,450 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:32,450 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:32,452 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:32,452 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:33,652 - DEBUG - Training batch loss: 6.953411818404948\n",
      "2025-11-02 13:45:33,652 - root - DEBUG - Training batch loss: 6.953411818404948\n",
      "2025-11-02 13:45:34,993 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:34,993 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:34,994 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:34,994 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:36,196 - DEBUG - Training batch loss: 7.269360753291077\n",
      "2025-11-02 13:45:36,196 - root - DEBUG - Training batch loss: 7.269360753291077\n",
      "2025-11-02 13:45:37,411 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:37,411 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:37,412 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:37,412 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:38,546 - DEBUG - Training batch loss: 7.243376246867816\n",
      "2025-11-02 13:45:38,546 - root - DEBUG - Training batch loss: 7.243376246867816\n",
      "2025-11-02 13:45:39,787 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:39,787 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:39,788 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:39,788 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:41,009 - DEBUG - Training batch loss: 6.918290982797194\n",
      "2025-11-02 13:45:41,009 - root - DEBUG - Training batch loss: 6.918290982797194\n",
      "2025-11-02 13:45:42,246 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:42,246 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:42,247 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:42,247 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:43,452 - DEBUG - Training batch loss: 7.658476844204731\n",
      "2025-11-02 13:45:43,452 - root - DEBUG - Training batch loss: 7.658476844204731\n",
      "2025-11-02 13:45:44,762 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:44,762 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:44,763 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:44,763 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:46,042 - DEBUG - Training batch loss: 6.516670278970006\n",
      "2025-11-02 13:45:46,042 - root - DEBUG - Training batch loss: 6.516670278970006\n",
      "2025-11-02 13:45:47,266 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:47,266 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:47,267 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:47,267 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:48,417 - DEBUG - Training batch loss: 6.660105341177848\n",
      "2025-11-02 13:45:48,417 - root - DEBUG - Training batch loss: 6.660105341177848\n",
      "2025-11-02 13:45:49,667 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:49,667 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:49,668 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:49,668 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:50,883 - DEBUG - Training batch loss: 6.92774464967049\n",
      "2025-11-02 13:45:50,883 - root - DEBUG - Training batch loss: 6.92774464967049\n",
      "2025-11-02 13:45:52,111 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:52,111 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:52,112 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:52,112 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:53,286 - DEBUG - Training batch loss: 6.653981737517174\n",
      "2025-11-02 13:45:53,286 - root - DEBUG - Training batch loss: 6.653981737517174\n",
      "2025-11-02 13:45:54,552 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:54,552 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:45:54,553 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:54,553 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:55,788 - DEBUG - Training batch loss: 7.468433115043448\n",
      "2025-11-02 13:45:55,788 - root - DEBUG - Training batch loss: 7.468433115043448\n",
      "2025-11-02 13:45:57,145 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:57,145 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:57,146 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:57,146 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:58,330 - DEBUG - Training batch loss: 7.035490853228092\n",
      "2025-11-02 13:45:58,330 - root - DEBUG - Training batch loss: 7.035490853228092\n",
      "2025-11-02 13:45:59,581 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:59,581 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:45:59,582 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:45:59,582 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:00,766 - DEBUG - Training batch loss: 6.99215938911639\n",
      "2025-11-02 13:46:00,766 - root - DEBUG - Training batch loss: 6.99215938911639\n",
      "2025-11-02 13:46:02,050 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:02,050 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:02,052 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:02,052 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:03,396 - DEBUG - Training batch loss: 6.894572508575948\n",
      "2025-11-02 13:46:03,396 - root - DEBUG - Training batch loss: 6.894572508575948\n",
      "2025-11-02 13:46:04,694 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:04,694 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:04,695 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:04,695 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:05,995 - DEBUG - Training batch loss: 6.952407816261266\n",
      "2025-11-02 13:46:05,995 - root - DEBUG - Training batch loss: 6.952407816261266\n",
      "2025-11-02 13:46:07,279 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:07,279 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:07,281 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:07,281 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:08,445 - DEBUG - Training batch loss: 6.77106358001938\n",
      "2025-11-02 13:46:08,445 - root - DEBUG - Training batch loss: 6.77106358001938\n",
      "2025-11-02 13:46:09,789 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:09,789 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:09,791 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:09,791 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:11,039 - DEBUG - Training batch loss: 7.154404061236824\n",
      "2025-11-02 13:46:11,039 - root - DEBUG - Training batch loss: 7.154404061236824\n",
      "2025-11-02 13:46:12,303 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:12,303 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:12,304 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:12,304 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:13,476 - DEBUG - Training batch loss: 7.344340207226882\n",
      "2025-11-02 13:46:13,476 - root - DEBUG - Training batch loss: 7.344340207226882\n",
      "2025-11-02 13:46:14,728 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:14,728 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:14,730 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:14,730 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:15,904 - DEBUG - Training batch loss: 7.040092654128876\n",
      "2025-11-02 13:46:15,904 - root - DEBUG - Training batch loss: 7.040092654128876\n",
      "2025-11-02 13:46:17,111 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:17,111 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:17,112 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:17,112 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:18,487 - DEBUG - Training batch loss: 6.98938866073649\n",
      "2025-11-02 13:46:18,487 - root - DEBUG - Training batch loss: 6.98938866073649\n",
      "2025-11-02 13:46:19,742 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:19,742 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:19,743 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:19,743 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:21,093 - DEBUG - Training batch loss: 6.495587271937016\n",
      "2025-11-02 13:46:21,093 - root - DEBUG - Training batch loss: 6.495587271937016\n",
      "2025-11-02 13:46:22,358 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:22,358 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:22,359 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:22,359 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:23,496 - DEBUG - Training batch loss: 6.976418120813472\n",
      "2025-11-02 13:46:23,496 - root - DEBUG - Training batch loss: 6.976418120813472\n",
      "2025-11-02 13:46:24,831 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:24,831 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:24,832 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:24,832 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:26,028 - DEBUG - Training batch loss: 6.044225682504579\n",
      "2025-11-02 13:46:26,028 - root - DEBUG - Training batch loss: 6.044225682504579\n",
      "2025-11-02 13:46:27,354 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:27,354 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:27,355 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:27,355 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:28,562 - DEBUG - Training batch loss: 8.014796570798758\n",
      "2025-11-02 13:46:28,562 - root - DEBUG - Training batch loss: 8.014796570798758\n",
      "2025-11-02 13:46:29,934 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:29,934 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:29,935 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:29,935 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:31,193 - DEBUG - Training batch loss: 6.867653940332881\n",
      "2025-11-02 13:46:31,193 - root - DEBUG - Training batch loss: 6.867653940332881\n",
      "2025-11-02 13:46:32,425 - DEBUG - Training batch accuracy: 0.09375\n",
      "2025-11-02 13:46:32,425 - root - DEBUG - Training batch accuracy: 0.09375\n",
      "2025-11-02 13:46:32,426 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:32,426 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:33,594 - DEBUG - Training batch loss: 7.84662219761257\n",
      "2025-11-02 13:46:33,594 - root - DEBUG - Training batch loss: 7.84662219761257\n",
      "2025-11-02 13:46:34,832 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:34,832 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:46:34,833 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:34,833 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:36,064 - DEBUG - Training batch loss: 7.528422205405933\n",
      "2025-11-02 13:46:36,064 - root - DEBUG - Training batch loss: 7.528422205405933\n",
      "2025-11-02 13:46:37,303 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:37,303 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:37,305 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:37,305 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:38,465 - DEBUG - Training batch loss: 6.146712637370597\n",
      "2025-11-02 13:46:38,465 - root - DEBUG - Training batch loss: 6.146712637370597\n",
      "2025-11-02 13:46:39,734 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:39,734 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:39,735 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:39,735 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:40,895 - DEBUG - Training batch loss: 6.725384038900871\n",
      "2025-11-02 13:46:40,895 - root - DEBUG - Training batch loss: 6.725384038900871\n",
      "2025-11-02 13:46:42,148 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:42,148 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:42,149 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:42,149 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:43,338 - DEBUG - Training batch loss: 7.389056274754854\n",
      "2025-11-02 13:46:43,338 - root - DEBUG - Training batch loss: 7.389056274754854\n",
      "2025-11-02 13:46:44,631 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:44,631 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:44,632 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:44,632 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:45,917 - DEBUG - Training batch loss: 7.519077450759385\n",
      "2025-11-02 13:46:45,917 - root - DEBUG - Training batch loss: 7.519077450759385\n",
      "2025-11-02 13:46:47,270 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:47,270 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:47,272 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:47,272 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:48,488 - DEBUG - Training batch loss: 7.643492607532468\n",
      "2025-11-02 13:46:48,488 - root - DEBUG - Training batch loss: 7.643492607532468\n",
      "2025-11-02 13:46:49,835 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:49,835 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:49,836 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:49,836 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:51,151 - DEBUG - Training batch loss: 6.986352038154546\n",
      "2025-11-02 13:46:51,151 - root - DEBUG - Training batch loss: 6.986352038154546\n",
      "2025-11-02 13:46:52,499 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:52,499 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:52,500 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:52,500 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:53,735 - DEBUG - Training batch loss: 7.004847774002654\n",
      "2025-11-02 13:46:53,735 - root - DEBUG - Training batch loss: 7.004847774002654\n",
      "2025-11-02 13:46:55,084 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:55,084 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:55,085 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:55,085 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:56,319 - DEBUG - Training batch loss: 7.107647873322291\n",
      "2025-11-02 13:46:56,319 - root - DEBUG - Training batch loss: 7.107647873322291\n",
      "2025-11-02 13:46:57,617 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:57,617 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:46:57,618 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:57,618 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:46:58,779 - DEBUG - Training batch loss: 7.127273271402638\n",
      "2025-11-02 13:46:58,779 - root - DEBUG - Training batch loss: 7.127273271402638\n",
      "2025-11-02 13:47:00,133 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:00,133 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:00,134 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:00,134 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:01,375 - DEBUG - Training batch loss: 8.005843293672601\n",
      "2025-11-02 13:47:01,375 - root - DEBUG - Training batch loss: 8.005843293672601\n",
      "2025-11-02 13:47:02,596 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:02,596 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:02,597 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:02,597 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:03,740 - DEBUG - Training batch loss: 7.146316302703875\n",
      "2025-11-02 13:47:03,740 - root - DEBUG - Training batch loss: 7.146316302703875\n",
      "2025-11-02 13:47:04,979 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:04,979 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:04,981 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:04,981 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:06,198 - DEBUG - Training batch loss: 7.120522284708042\n",
      "2025-11-02 13:47:06,198 - root - DEBUG - Training batch loss: 7.120522284708042\n",
      "2025-11-02 13:47:07,389 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:07,389 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:07,390 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:07,390 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:08,539 - DEBUG - Training batch loss: 6.500604994461228\n",
      "2025-11-02 13:47:08,539 - root - DEBUG - Training batch loss: 6.500604994461228\n",
      "2025-11-02 13:47:09,769 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:09,769 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:09,770 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:09,770 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:11,077 - DEBUG - Training batch loss: 6.603643641240364\n",
      "2025-11-02 13:47:11,077 - root - DEBUG - Training batch loss: 6.603643641240364\n",
      "2025-11-02 13:47:12,290 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:12,290 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:12,291 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:12,291 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:13,425 - DEBUG - Training batch loss: 8.166271371223008\n",
      "2025-11-02 13:47:13,425 - root - DEBUG - Training batch loss: 8.166271371223008\n",
      "2025-11-02 13:47:14,695 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:14,695 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:14,696 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:14,696 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:15,899 - DEBUG - Training batch loss: 7.035353045908531\n",
      "2025-11-02 13:47:15,899 - root - DEBUG - Training batch loss: 7.035353045908531\n",
      "2025-11-02 13:47:17,100 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:17,100 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:17,101 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:17,101 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:18,239 - DEBUG - Training batch loss: 7.76994766862572\n",
      "2025-11-02 13:47:18,239 - root - DEBUG - Training batch loss: 7.76994766862572\n",
      "2025-11-02 13:47:19,469 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:19,469 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:19,470 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:19,470 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:20,667 - DEBUG - Training batch loss: 7.236513802678236\n",
      "2025-11-02 13:47:20,667 - root - DEBUG - Training batch loss: 7.236513802678236\n",
      "2025-11-02 13:47:21,905 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:21,905 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:21,906 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:21,906 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:23,025 - DEBUG - Training batch loss: 7.011224301108704\n",
      "2025-11-02 13:47:23,025 - root - DEBUG - Training batch loss: 7.011224301108704\n",
      "2025-11-02 13:47:24,253 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:24,253 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:24,255 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:24,255 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:25,449 - DEBUG - Training batch loss: 5.611417671969743\n",
      "2025-11-02 13:47:25,449 - root - DEBUG - Training batch loss: 5.611417671969743\n",
      "2025-11-02 13:47:26,687 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:26,687 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:26,689 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:26,689 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:27,832 - DEBUG - Training batch loss: 7.654661840463162\n",
      "2025-11-02 13:47:27,832 - root - DEBUG - Training batch loss: 7.654661840463162\n",
      "2025-11-02 13:47:29,070 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:29,070 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:29,071 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:29,071 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:30,274 - DEBUG - Training batch loss: 7.053071624475528\n",
      "2025-11-02 13:47:30,274 - root - DEBUG - Training batch loss: 7.053071624475528\n",
      "2025-11-02 13:47:31,480 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:31,480 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:31,481 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:31,481 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:32,621 - DEBUG - Training batch loss: 7.499462237207085\n",
      "2025-11-02 13:47:32,621 - root - DEBUG - Training batch loss: 7.499462237207085\n",
      "2025-11-02 13:47:33,928 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:33,928 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:33,930 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:33,930 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:35,097 - DEBUG - Training batch loss: 7.751924795363988\n",
      "2025-11-02 13:47:35,097 - root - DEBUG - Training batch loss: 7.751924795363988\n",
      "2025-11-02 13:47:36,295 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:36,295 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:36,296 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:36,296 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:37,390 - DEBUG - Training batch loss: 7.506535020582356\n",
      "2025-11-02 13:47:37,390 - root - DEBUG - Training batch loss: 7.506535020582356\n",
      "2025-11-02 13:47:38,647 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:38,647 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:38,649 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:38,649 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:39,811 - DEBUG - Training batch loss: 7.587041060568751\n",
      "2025-11-02 13:47:39,811 - root - DEBUG - Training batch loss: 7.587041060568751\n",
      "2025-11-02 13:47:41,019 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:41,019 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:41,020 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:41,020 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:42,156 - DEBUG - Training batch loss: 7.292409578497784\n",
      "2025-11-02 13:47:42,156 - root - DEBUG - Training batch loss: 7.292409578497784\n",
      "2025-11-02 13:47:43,396 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:43,396 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:43,397 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:43,397 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:44,604 - DEBUG - Training batch loss: 6.8458764939626535\n",
      "2025-11-02 13:47:44,604 - root - DEBUG - Training batch loss: 6.8458764939626535\n",
      "2025-11-02 13:47:45,794 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:45,794 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:45,795 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:45,795 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:46,924 - DEBUG - Training batch loss: 7.395457553327047\n",
      "2025-11-02 13:47:46,924 - root - DEBUG - Training batch loss: 7.395457553327047\n",
      "2025-11-02 13:47:48,175 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:48,175 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:48,177 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:48,177 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:49,361 - DEBUG - Training batch loss: 7.482978087999507\n",
      "2025-11-02 13:47:49,361 - root - DEBUG - Training batch loss: 7.482978087999507\n",
      "2025-11-02 13:47:50,558 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:50,558 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:50,559 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:50,559 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:51,704 - DEBUG - Training batch loss: 7.176922915944196\n",
      "2025-11-02 13:47:51,704 - root - DEBUG - Training batch loss: 7.176922915944196\n",
      "2025-11-02 13:47:52,953 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:52,953 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:52,954 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:52,954 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:54,172 - DEBUG - Training batch loss: 6.470149265624519\n",
      "2025-11-02 13:47:54,172 - root - DEBUG - Training batch loss: 6.470149265624519\n",
      "2025-11-02 13:47:55,408 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:55,408 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:47:55,409 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:55,409 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:56,600 - DEBUG - Training batch loss: 6.7766981804258135\n",
      "2025-11-02 13:47:56,600 - root - DEBUG - Training batch loss: 6.7766981804258135\n",
      "2025-11-02 13:47:57,847 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:57,847 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:47:57,848 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:57,848 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:47:59,020 - DEBUG - Training batch loss: 6.724566453038067\n",
      "2025-11-02 13:47:59,020 - root - DEBUG - Training batch loss: 6.724566453038067\n",
      "2025-11-02 13:48:00,219 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:00,219 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:00,220 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:00,220 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:01,345 - DEBUG - Training batch loss: 7.045609496324108\n",
      "2025-11-02 13:48:01,345 - root - DEBUG - Training batch loss: 7.045609496324108\n",
      "2025-11-02 13:48:02,576 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:02,576 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:02,577 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:02,577 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:03,775 - DEBUG - Training batch loss: 7.398399393237168\n",
      "2025-11-02 13:48:03,775 - root - DEBUG - Training batch loss: 7.398399393237168\n",
      "2025-11-02 13:48:04,983 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:04,983 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:04,984 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:04,984 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:06,140 - DEBUG - Training batch loss: 6.940205871477989\n",
      "2025-11-02 13:48:06,140 - root - DEBUG - Training batch loss: 6.940205871477989\n",
      "2025-11-02 13:48:07,368 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:07,368 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:07,370 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:07,370 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:08,527 - DEBUG - Training batch loss: 7.483411305062558\n",
      "2025-11-02 13:48:08,527 - root - DEBUG - Training batch loss: 7.483411305062558\n",
      "2025-11-02 13:48:09,733 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:09,733 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:09,734 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:09,734 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:10,857 - DEBUG - Training batch loss: 7.767738936156817\n",
      "2025-11-02 13:48:10,857 - root - DEBUG - Training batch loss: 7.767738936156817\n",
      "2025-11-02 13:48:12,102 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:12,102 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:12,104 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:12,104 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:13,303 - DEBUG - Training batch loss: 6.976599739389622\n",
      "2025-11-02 13:48:13,303 - root - DEBUG - Training batch loss: 6.976599739389622\n",
      "2025-11-02 13:48:14,494 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:14,494 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:14,495 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:14,495 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:15,649 - DEBUG - Training batch loss: 6.653396036330836\n",
      "2025-11-02 13:48:15,649 - root - DEBUG - Training batch loss: 6.653396036330836\n",
      "2025-11-02 13:48:16,919 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:16,919 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:16,920 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:16,920 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:18,093 - DEBUG - Training batch loss: 7.1764658756713615\n",
      "2025-11-02 13:48:18,093 - root - DEBUG - Training batch loss: 7.1764658756713615\n",
      "2025-11-02 13:48:19,302 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:19,302 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:19,303 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:19,303 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:20,472 - DEBUG - Training batch loss: 7.6167348550648075\n",
      "2025-11-02 13:48:20,472 - root - DEBUG - Training batch loss: 7.6167348550648075\n",
      "2025-11-02 13:48:21,728 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:21,728 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:21,729 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:21,729 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:22,880 - DEBUG - Training batch loss: 7.1412265072182795\n",
      "2025-11-02 13:48:22,880 - root - DEBUG - Training batch loss: 7.1412265072182795\n",
      "2025-11-02 13:48:24,075 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:24,075 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:24,076 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:24,076 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:25,155 - DEBUG - Training batch loss: 7.131242708612291\n",
      "2025-11-02 13:48:25,155 - root - DEBUG - Training batch loss: 7.131242708612291\n",
      "2025-11-02 13:48:26,415 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:26,415 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:26,416 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:26,416 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:27,541 - DEBUG - Training batch loss: 6.542091138109536\n",
      "2025-11-02 13:48:27,541 - root - DEBUG - Training batch loss: 6.542091138109536\n",
      "2025-11-02 13:48:28,743 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:28,743 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:28,744 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:28,744 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:29,890 - DEBUG - Training batch loss: 6.57733092137752\n",
      "2025-11-02 13:48:29,890 - root - DEBUG - Training batch loss: 6.57733092137752\n",
      "2025-11-02 13:48:31,130 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:31,130 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:31,131 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:31,131 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:32,348 - DEBUG - Training batch loss: 6.9539150571091435\n",
      "2025-11-02 13:48:32,348 - root - DEBUG - Training batch loss: 6.9539150571091435\n",
      "2025-11-02 13:48:33,577 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:33,577 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:33,578 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:33,578 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:34,684 - DEBUG - Training batch loss: 7.014329874024135\n",
      "2025-11-02 13:48:34,684 - root - DEBUG - Training batch loss: 7.014329874024135\n",
      "2025-11-02 13:48:35,971 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:35,971 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:35,972 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:35,972 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:37,167 - DEBUG - Training batch loss: 7.773381060914649\n",
      "2025-11-02 13:48:37,167 - root - DEBUG - Training batch loss: 7.773381060914649\n",
      "2025-11-02 13:48:38,369 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:38,369 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:38,370 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:38,370 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:39,522 - DEBUG - Training batch loss: 6.037743555660201\n",
      "2025-11-02 13:48:39,522 - root - DEBUG - Training batch loss: 6.037743555660201\n",
      "2025-11-02 13:48:40,755 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:48:40,755 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:48:40,756 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:40,756 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:41,944 - DEBUG - Training batch loss: 6.598712542260181\n",
      "2025-11-02 13:48:41,944 - root - DEBUG - Training batch loss: 6.598712542260181\n",
      "2025-11-02 13:48:43,178 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:43,178 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:43,180 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:43,180 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:44,326 - DEBUG - Training batch loss: 6.917221605305302\n",
      "2025-11-02 13:48:44,326 - root - DEBUG - Training batch loss: 6.917221605305302\n",
      "2025-11-02 13:48:45,583 - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:48:45,583 - root - DEBUG - Training batch accuracy: 0.0625\n",
      "2025-11-02 13:48:45,584 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:45,584 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:46,792 - DEBUG - Training batch loss: 7.385552075919442\n",
      "2025-11-02 13:48:46,792 - root - DEBUG - Training batch loss: 7.385552075919442\n",
      "2025-11-02 13:48:48,006 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:48,006 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:48,007 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:48,007 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:49,161 - DEBUG - Training batch loss: 7.293950529352205\n",
      "2025-11-02 13:48:49,161 - root - DEBUG - Training batch loss: 7.293950529352205\n",
      "2025-11-02 13:48:50,394 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:50,394 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:50,395 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:50,395 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:51,597 - DEBUG - Training batch loss: 6.619820611610818\n",
      "2025-11-02 13:48:51,597 - root - DEBUG - Training batch loss: 6.619820611610818\n",
      "2025-11-02 13:48:52,844 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:52,844 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:52,846 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:52,846 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:53,998 - DEBUG - Training batch loss: 6.856480372183785\n",
      "2025-11-02 13:48:53,998 - root - DEBUG - Training batch loss: 6.856480372183785\n",
      "2025-11-02 13:48:55,241 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:55,241 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:48:55,242 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:55,242 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:56,452 - DEBUG - Training batch loss: 6.990542801152149\n",
      "2025-11-02 13:48:56,452 - root - DEBUG - Training batch loss: 6.990542801152149\n",
      "2025-11-02 13:48:57,719 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:57,719 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:48:57,720 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:57,720 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:48:58,861 - DEBUG - Training batch loss: 6.869619053818344\n",
      "2025-11-02 13:48:58,861 - root - DEBUG - Training batch loss: 6.869619053818344\n",
      "2025-11-02 13:49:00,112 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:00,112 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:00,113 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:00,113 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:01,294 - DEBUG - Training batch loss: 7.471919905939654\n",
      "2025-11-02 13:49:01,294 - root - DEBUG - Training batch loss: 7.471919905939654\n",
      "2025-11-02 13:49:02,500 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:49:02,500 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:49:02,501 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:02,501 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:03,629 - DEBUG - Training batch loss: 6.5204635578817935\n",
      "2025-11-02 13:49:03,629 - root - DEBUG - Training batch loss: 6.5204635578817935\n",
      "2025-11-02 13:49:04,953 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:04,953 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:04,954 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:04,954 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:06,153 - DEBUG - Training batch loss: 7.106233863478621\n",
      "2025-11-02 13:49:06,153 - root - DEBUG - Training batch loss: 7.106233863478621\n",
      "2025-11-02 13:49:07,374 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:07,374 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:07,376 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:07,376 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:08,573 - DEBUG - Training batch loss: 6.551368484272101\n",
      "2025-11-02 13:49:08,573 - root - DEBUG - Training batch loss: 6.551368484272101\n",
      "2025-11-02 13:49:09,894 - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:49:09,894 - root - DEBUG - Training batch accuracy: 0.03125\n",
      "2025-11-02 13:49:09,895 - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:09,895 - root - DEBUG - Processing training batch with 32 samples.\n",
      "2025-11-02 13:49:11,075 - DEBUG - Training batch loss: 6.574695158530488\n",
      "2025-11-02 13:49:11,075 - root - DEBUG - Training batch loss: 6.574695158530488\n",
      "2025-11-02 13:49:12,264 - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:12,264 - root - DEBUG - Training batch accuracy: 0.0\n",
      "2025-11-02 13:49:12,265 - INFO - Epoch 3/3, Train Loss: 7.1020,Train Accuracy: 0.0094\n",
      "2025-11-02 13:49:12,265 - root - INFO - Epoch 3/3, Train Loss: 7.1020,Train Accuracy: 0.0094\n",
      "2025-11-02 13:49:12,301 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:12,301 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:13,468 - DEBUG - Validation batch loss: 15.372261651047438\n",
      "2025-11-02 13:49:13,468 - root - DEBUG - Validation batch loss: 15.372261651047438\n",
      "2025-11-02 13:49:13,469 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:13,469 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:13,470 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:13,470 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:14,677 - DEBUG - Validation batch loss: 26.599377176732947\n",
      "2025-11-02 13:49:14,677 - root - DEBUG - Validation batch loss: 26.599377176732947\n",
      "2025-11-02 13:49:14,679 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:14,679 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:14,680 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:14,680 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:15,823 - DEBUG - Validation batch loss: 28.149246341814035\n",
      "2025-11-02 13:49:15,823 - root - DEBUG - Validation batch loss: 28.149246341814035\n",
      "2025-11-02 13:49:15,825 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:15,825 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:15,826 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:15,826 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:17,005 - DEBUG - Validation batch loss: 28.149246341814035\n",
      "2025-11-02 13:49:17,005 - root - DEBUG - Validation batch loss: 28.149246341814035\n",
      "2025-11-02 13:49:17,009 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:17,009 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:17,011 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:17,011 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:18,222 - DEBUG - Validation batch loss: 27.7021074469736\n",
      "2025-11-02 13:49:18,222 - root - DEBUG - Validation batch loss: 27.7021074469736\n",
      "2025-11-02 13:49:18,225 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:18,225 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:18,227 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:18,227 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:19,389 - DEBUG - Validation batch loss: 14.29615382260591\n",
      "2025-11-02 13:49:19,389 - root - DEBUG - Validation batch loss: 14.29615382260591\n",
      "2025-11-02 13:49:19,390 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:19,390 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:19,391 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:19,391 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:20,597 - DEBUG - Validation batch loss: 9.902238096688402\n",
      "2025-11-02 13:49:20,597 - root - DEBUG - Validation batch loss: 9.902238096688402\n",
      "2025-11-02 13:49:20,598 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:20,598 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:20,599 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:20,599 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:21,833 - DEBUG - Validation batch loss: 9.752572094175058\n",
      "2025-11-02 13:49:21,833 - root - DEBUG - Validation batch loss: 9.752572094175058\n",
      "2025-11-02 13:49:21,835 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:21,835 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:21,838 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:21,838 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:23,046 - DEBUG - Validation batch loss: 7.901197806852817\n",
      "2025-11-02 13:49:23,046 - root - DEBUG - Validation batch loss: 7.901197806852817\n",
      "2025-11-02 13:49:23,050 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:23,050 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:23,050 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:23,050 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:24,197 - DEBUG - Validation batch loss: 21.778119260960505\n",
      "2025-11-02 13:49:24,197 - root - DEBUG - Validation batch loss: 21.778119260960505\n",
      "2025-11-02 13:49:24,198 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:24,198 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:24,199 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:24,199 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:25,380 - DEBUG - Validation batch loss: 12.16877844493483\n",
      "2025-11-02 13:49:25,380 - root - DEBUG - Validation batch loss: 12.16877844493483\n",
      "2025-11-02 13:49:25,381 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:25,381 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:25,381 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:25,381 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:26,637 - DEBUG - Validation batch loss: 16.768977112827127\n",
      "2025-11-02 13:49:26,637 - root - DEBUG - Validation batch loss: 16.768977112827127\n",
      "2025-11-02 13:49:26,642 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:26,642 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:26,643 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:26,643 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:27,844 - DEBUG - Validation batch loss: 32.855736416359434\n",
      "2025-11-02 13:49:27,844 - root - DEBUG - Validation batch loss: 32.855736416359434\n",
      "2025-11-02 13:49:27,846 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:27,846 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:27,847 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:27,847 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:29,001 - DEBUG - Validation batch loss: 13.59213143935738\n",
      "2025-11-02 13:49:29,001 - root - DEBUG - Validation batch loss: 13.59213143935738\n",
      "2025-11-02 13:49:29,002 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:29,002 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:29,003 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:29,003 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:30,190 - DEBUG - Validation batch loss: 19.688896957757635\n",
      "2025-11-02 13:49:30,190 - root - DEBUG - Validation batch loss: 19.688896957757635\n",
      "2025-11-02 13:49:30,193 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:30,193 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:30,196 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:30,196 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:31,417 - DEBUG - Validation batch loss: 35.31523054220979\n",
      "2025-11-02 13:49:31,417 - root - DEBUG - Validation batch loss: 35.31523054220979\n",
      "2025-11-02 13:49:31,419 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:31,419 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:31,419 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:31,419 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:32,636 - DEBUG - Validation batch loss: 14.193525120521155\n",
      "2025-11-02 13:49:32,636 - root - DEBUG - Validation batch loss: 14.193525120521155\n",
      "2025-11-02 13:49:32,637 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:32,637 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:32,638 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:32,638 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:33,780 - DEBUG - Validation batch loss: 13.102771914372886\n",
      "2025-11-02 13:49:33,780 - root - DEBUG - Validation batch loss: 13.102771914372886\n",
      "2025-11-02 13:49:33,781 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:33,781 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:33,782 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:33,782 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:35,027 - DEBUG - Validation batch loss: 11.330573460356042\n",
      "2025-11-02 13:49:35,027 - root - DEBUG - Validation batch loss: 11.330573460356042\n",
      "2025-11-02 13:49:35,029 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:35,029 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:35,031 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:35,031 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:36,256 - DEBUG - Validation batch loss: 15.12294525593017\n",
      "2025-11-02 13:49:36,256 - root - DEBUG - Validation batch loss: 15.12294525593017\n",
      "2025-11-02 13:49:36,257 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:36,257 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:36,258 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:36,258 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:37,400 - DEBUG - Validation batch loss: 20.245913677688158\n",
      "2025-11-02 13:49:37,400 - root - DEBUG - Validation batch loss: 20.245913677688158\n",
      "2025-11-02 13:49:37,401 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:37,401 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:37,401 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:37,401 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:38,584 - DEBUG - Validation batch loss: 21.815167640028186\n",
      "2025-11-02 13:49:38,584 - root - DEBUG - Validation batch loss: 21.815167640028186\n",
      "2025-11-02 13:49:38,585 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:38,585 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:38,586 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:38,586 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:39,777 - DEBUG - Validation batch loss: 39.17519300468186\n",
      "2025-11-02 13:49:39,777 - root - DEBUG - Validation batch loss: 39.17519300468186\n",
      "2025-11-02 13:49:39,778 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:39,778 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:39,779 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:39,779 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:40,928 - DEBUG - Validation batch loss: 20.57803995473673\n",
      "2025-11-02 13:49:40,928 - root - DEBUG - Validation batch loss: 20.57803995473673\n",
      "2025-11-02 13:49:40,929 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:40,929 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:40,929 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:40,929 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:42,073 - DEBUG - Validation batch loss: 22.75574999032965\n",
      "2025-11-02 13:49:42,073 - root - DEBUG - Validation batch loss: 22.75574999032965\n",
      "2025-11-02 13:49:42,074 - DEBUG - Validation batch accuracy: 0.03125\n",
      "2025-11-02 13:49:42,074 - root - DEBUG - Validation batch accuracy: 0.03125\n",
      "2025-11-02 13:49:42,074 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:42,074 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:43,208 - DEBUG - Validation batch loss: 17.163810399976242\n",
      "2025-11-02 13:49:43,208 - root - DEBUG - Validation batch loss: 17.163810399976242\n",
      "2025-11-02 13:49:43,209 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:43,209 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:43,210 - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:43,210 - root - DEBUG - Processing validation batch with 32 samples.\n",
      "2025-11-02 13:49:44,410 - DEBUG - Validation batch loss: 46.992717760627855\n",
      "2025-11-02 13:49:44,410 - root - DEBUG - Validation batch loss: 46.992717760627855\n",
      "2025-11-02 13:49:44,411 - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:44,411 - root - DEBUG - Validation batch accuracy: 0.0\n",
      "2025-11-02 13:49:44,411 - INFO - Epoch 3/3,Val Loss: 20.8322, Val Accuracy: 0.0012\n",
      "2025-11-02 13:49:44,411 - root - INFO - Epoch 3/3,Val Loss: 20.8322, Val Accuracy: 0.0012\n",
      "2025-11-02 13:49:44,651 - DEBUG - Checkpoint saved to checkpoints/model_epoch_3.npz\n",
      "2025-11-02 13:49:44,651 - root - DEBUG - Checkpoint saved to checkpoints/model_epoch_3.npz\n",
      "2025-11-02 13:49:44,652 - DEBUG - Training completed.\n",
      "2025-11-02 13:49:44,652 - root - DEBUG - Training completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model using the train function\n",
    "train_losses, val_losses, train_accs, val_accs = train(\n",
    "    model, train_loader, val_loader, loss_fn, optimizer, num_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e59078-fd11-4c3c-9310-9c6375ee1569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(7.224014663241864), np.float64(7.150945399221631), np.float64(7.101973547847654)]\n",
      "[np.float64(20.295351623992648), np.float64(20.832118129483334), np.float64(20.832173301198516)]\n",
      "[np.float64(0.013973577235772357), np.float64(0.012703252032520325), np.float64(0.009400406504065041)]\n",
      "[np.float64(0.0011574074074074073), np.float64(0.0011574074074074073), np.float64(0.0011574074074074073)]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses)\n",
    "print(val_losses)\n",
    "print(train_accs)\n",
    "print(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca0bb96a-7a46-4ba5-b176-79d5ed6bff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 24.9288, Test Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import evaluate\n",
    "\n",
    "importlib.reload(evaluate)  # reloads the updated file\n",
    "\n",
    "test_loss, test_acc = evaluate.evaluate(model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53314b89-e1bc-49a9-9e19-9f20f98aa93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 24.9288, Test Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfb6c0f4-6896-4b5b-b477-90c193db7c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 14:19:18,778 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansManichaean-Regular.ttf', name='Noto Sans Manichaean', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,778 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansManichaean-Regular.ttf', name='Noto Sans Manichaean', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,778 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSinhalaUI-Bold.ttf', name='Noto Sans Sinhala UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,778 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSinhalaUI-Bold.ttf', name='Noto Sans Sinhala UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,779 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,779 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Bold.ttf', name='Liberation Serif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,784 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSiddham-Regular.ttf', name='Noto Sans Siddham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,784 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSiddham-Regular.ttf', name='Noto Sans Siddham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,785 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/hack/Hack-Italic.ttf', name='Hack', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,785 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/hack/Hack-Italic.ttf', name='Hack', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,786 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansCoptic-Regular.ttf', name='Noto Sans Coptic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,786 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansCoptic-Regular.ttf', name='Noto Sans Coptic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,787 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2025-11-02 14:19:18,787 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2025-11-02 14:19:18,788 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Fraktur-Bold.ttf', name='KaTeX_Fraktur', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,788 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Fraktur-Bold.ttf', name='KaTeX_Fraktur', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusRoman-Regular.otf', name='Nimbus Roman', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,789 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusRoman-Regular.otf', name='Nimbus Roman', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,789 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTibetan-Bold.ttf', name='Noto Serif Tibetan', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,789 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTibetan-Bold.ttf', name='Noto Serif Tibetan', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,790 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMahajani-Regular.ttf', name='Noto Sans Mahajani', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,790 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMahajani-Regular.ttf', name='Noto Sans Mahajani', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,791 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamil-Regular.ttf', name='Noto Serif Tamil', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,791 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamil-Regular.ttf', name='Noto Serif Tamil', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,796 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLaoUI-Regular.ttf', name='Noto Sans Lao UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,796 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLaoUI-Regular.ttf', name='Noto Sans Lao UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifSinhala-Bold.ttf', name='Noto Serif Sinhala', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,798 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifSinhala-Bold.ttf', name='Noto Serif Sinhala', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,798 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifMalayalam-Bold.ttf', name='Noto Serif Malayalam', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,798 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifMalayalam-Bold.ttf', name='Noto Serif Malayalam', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKaithi-Regular.ttf', name='Noto Sans Kaithi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,799 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKaithi-Regular.ttf', name='Noto Sans Kaithi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,799 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedThaiUI-Bold.ttf', name='Noto Looped Thai UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,799 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedThaiUI-Bold.ttf', name='Noto Looped Thai UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,800 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMeeteiMayek-Regular.ttf', name='Noto Sans Meetei Mayek', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,800 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMeeteiMayek-Regular.ttf', name='Noto Sans Meetei Mayek', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,801 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25\n",
      "2025-11-02 14:19:18,801 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Italic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25\n",
      "2025-11-02 14:19:18,802 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansNewa-Regular.ttf', name='Noto Sans Newa', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,802 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansNewa-Regular.ttf', name='Noto Sans Newa', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,804 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGujaratiUI-Bold.ttf', name='Noto Sans Gujarati UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,804 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGujaratiUI-Bold.ttf', name='Noto Sans Gujarati UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,804 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535\n",
      "2025-11-02 14:19:18,804 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Bold.ttf', name='DejaVu Serif', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535\n",
      "2025-11-02 14:19:18,805 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGeorgian-Bold.ttf', name='Noto Serif Georgian', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,805 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGeorgian-Bold.ttf', name='Noto Serif Georgian', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,806 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf', name='Noto Sans Hebrew', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,806 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf', name='Noto Sans Hebrew', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,807 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansAvestan-Regular.ttf', name='Noto Sans Avestan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,807 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansAvestan-Regular.ttf', name='Noto Sans Avestan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,807 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLimbu-Regular.ttf', name='Noto Sans Limbu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,807 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLimbu-Regular.ttf', name='Noto Sans Limbu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,808 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLycian-Regular.ttf', name='Noto Sans Lycian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,808 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLycian-Regular.ttf', name='Noto Sans Lycian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,809 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTaiTham-Bold.ttf', name='Noto Sans Tai Tham', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,809 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTaiTham-Bold.ttf', name='Noto Sans Tai Tham', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,810 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOriyaUI-Bold.ttf', name='Noto Sans Oriya UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,810 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOriyaUI-Bold.ttf', name='Noto Sans Oriya UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,811 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghSIL-Regular.ttf', name='Noto Sans Tifinagh SIL', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,811 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghSIL-Regular.ttf', name='Noto Sans Tifinagh SIL', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,812 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,812 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-BoldItalic.ttf', name='Liberation Mono', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,813 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu[wdth,wght].ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,813 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu[wdth,wght].ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,814 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2025-11-02 14:19:18,814 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSansNarrow-BoldItalic.ttf', name='Liberation Sans Narrow', style='italic', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2025-11-02 14:19:18,815 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansJavanese-Bold.ttf', name='Noto Sans Javanese', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,815 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansJavanese-Bold.ttf', name='Noto Sans Javanese', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,816 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhojki-Regular.ttf', name='Noto Sans Khojki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,816 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhojki-Regular.ttf', name='Noto Sans Khojki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,817 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDevanagari-Regular.ttf', name='Noto Serif Devanagari', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,817 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDevanagari-Regular.ttf', name='Noto Serif Devanagari', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,823 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamilSlanted-Bold.ttf', name='Noto Serif Tamil Slanted', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,823 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamilSlanted-Bold.ttf', name='Noto Serif Tamil Slanted', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,824 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKannadaUI-Bold.ttf', name='Noto Sans Kannada UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,824 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKannadaUI-Bold.ttf', name='Noto Sans Kannada UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,825 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLinearB-Regular.ttf', name='Noto Sans Linear B', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,825 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLinearB-Regular.ttf', name='Noto Sans Linear B', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,826 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Typewriter-Regular.ttf', name='KaTeX_Typewriter', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,826 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Typewriter-Regular.ttf', name='KaTeX_Typewriter', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,827 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDisplay-Italic.ttf', name='Noto Sans Display', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,827 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDisplay-Italic.ttf', name='Noto Sans Display', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,829 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansEthiopic-Regular.ttf', name='Noto Sans Ethiopic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,829 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansEthiopic-Regular.ttf', name='Noto Sans Ethiopic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,830 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghAgrawImazighen-Regular.ttf', name='Noto Sans Tifinagh Agraw Imazighen', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,830 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghAgrawImazighen-Regular.ttf', name='Noto Sans Tifinagh Agraw Imazighen', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,831 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHanunoo-Regular.ttf', name='Noto Sans Hanunoo', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,831 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHanunoo-Regular.ttf', name='Noto Sans Hanunoo', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,832 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifMalayalam-Regular.ttf', name='Noto Serif Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,832 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifMalayalam-Regular.ttf', name='Noto Serif Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,838 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25\n",
      "2025-11-02 14:19:18,838 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerifCondensed-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='condensed', size='scalable')) = 11.25\n",
      "2025-11-02 14:19:18,840 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBalinese-Bold.ttf', name='Noto Sans Balinese', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,840 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBalinese-Bold.ttf', name='Noto Sans Balinese', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,841 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKayahLi-Bold.ttf', name='Noto Sans Kayah Li', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,841 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKayahLi-Bold.ttf', name='Noto Sans Kayah Li', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,842 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSinhala-Regular.ttf', name='Noto Sans Sinhala', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,842 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSinhala-Regular.ttf', name='Noto Sans Sinhala', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,843 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25\n",
      "2025-11-02 14:19:18,843 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='condensed', size='scalable')) = 1.25\n",
      "2025-11-02 14:19:18,844 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansCham-Regular.ttf', name='Noto Sans Cham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,844 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansCham-Regular.ttf', name='Noto Sans Cham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,845 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoMusic-Regular.ttf', name='Noto Music', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,845 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoMusic-Regular.ttf', name='Noto Music', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,846 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,846 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-BoldItalic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,847 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,847 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Bold.ttf', name='DejaVu Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,848 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifYezidi-Bold.ttf', name='Noto Serif Yezidi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,848 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifYezidi-Bold.ttf', name='Noto Serif Yezidi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,849 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansPahawhHmong-Regular.ttf', name='Noto Sans Pahawh Hmong', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,849 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansPahawhHmong-Regular.ttf', name='Noto Sans Pahawh Hmong', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,850 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMultani-Regular.ttf', name='Noto Sans Multani', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,850 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMultani-Regular.ttf', name='Noto Sans Multani', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,850 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSinhala-Bold.ttf', name='Noto Sans Sinhala', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,850 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSinhala-Bold.ttf', name='Noto Sans Sinhala', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,851 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGujarati-Regular.ttf', name='Noto Sans Gujarati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,851 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGujarati-Regular.ttf', name='Noto Sans Gujarati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,852 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSans-Italic.ttf', name='Noto Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,852 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSans-Italic.ttf', name='Noto Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,853 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,853 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Italic.ttf', name='Liberation Sans', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,854 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-MI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,854 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-MI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,859 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBrahmi-Regular.ttf', name='Noto Sans Brahmi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,859 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBrahmi-Regular.ttf', name='Noto Sans Brahmi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,860 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansPsalterPahlavi-Regular.ttf', name='Noto Sans Psalter Pahlavi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,860 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansPsalterPahlavi-Regular.ttf', name='Noto Sans Psalter Pahlavi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,861 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSharada-Regular.ttf', name='Noto Sans Sharada', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,861 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSharada-Regular.ttf', name='Noto Sans Sharada', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,862 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansInscriptionalParthian-Regular.ttf', name='Noto Sans Inscriptional Parthian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,862 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansInscriptionalParthian-Regular.ttf', name='Noto Sans Inscriptional Parthian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,863 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansIndicSiyaqNumbers-Regular.ttf', name='Noto Sans Indic Siyaq Numbers', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,863 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansIndicSiyaqNumbers-Regular.ttf', name='Noto Sans Indic Siyaq Numbers', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,864 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMongolian-Regular.ttf', name='Noto Sans Mongolian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,864 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMongolian-Regular.ttf', name='Noto Sans Mongolian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,865 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuSansMono[wght].ttf', name='Ubuntu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,865 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuSansMono[wght].ttf', name='Ubuntu Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,866 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifBengali-Bold.ttf', name='Noto Serif Bengali', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,866 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifBengali-Bold.ttf', name='Noto Serif Bengali', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,867 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Math-BoldItalic.ttf', name='KaTeX_Math', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,867 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Math-BoldItalic.ttf', name='KaTeX_Math', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,871 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKhojki-Regular.ttf', name='Noto Serif Khojki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,871 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKhojki-Regular.ttf', name='Noto Serif Khojki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,872 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansThaana-Regular.ttf', name='Noto Sans Thaana', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,872 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansThaana-Regular.ttf', name='Noto Sans Thaana', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,873 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc', name='Noto Sans CJK JP', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,873 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Bold.ttc', name='Noto Sans CJK JP', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,874 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Size4-Regular.ttf', name='KaTeX_Size4', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,874 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Size4-Regular.ttf', name='KaTeX_Size4', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,876 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTaiLe-Regular.ttf', name='Noto Sans Tai Le', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,876 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTaiLe-Regular.ttf', name='Noto Sans Tai Le', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,882 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_SansSerif-Italic.ttf', name='KaTeX_SansSerif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,882 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_SansSerif-Italic.ttf', name='KaTeX_SansSerif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,883 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGrantha-Regular.ttf', name='Noto Sans Grantha', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,883 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGrantha-Regular.ttf', name='Noto Sans Grantha', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,884 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-R.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,884 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-R.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,885 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/P052-Italic.otf', name='P052', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,885 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/P052-Italic.otf', name='P052', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,886 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/libreoffice/opens___.ttf', name='OpenSymbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,886 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/libreoffice/opens___.ttf', name='OpenSymbol', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,886 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoRashiHebrew-Regular.ttf', name='Noto Rashi Hebrew', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,886 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoRashiHebrew-Regular.ttf', name='Noto Rashi Hebrew', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,887 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSans-BoldItalic.otf', name='Nimbus Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,887 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSans-BoldItalic.otf', name='Nimbus Sans', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,888 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLepcha-Regular.ttf', name='Noto Sans Lepcha', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,888 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLepcha-Regular.ttf', name='Noto Sans Lepcha', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,889 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono[wght].ttf', name='Ubuntu Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,889 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono[wght].ttf', name='Ubuntu Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,890 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,890 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-B.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,891 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDeseret-Regular.ttf', name='Noto Sans Deseret', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,891 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDeseret-Regular.ttf', name='Noto Sans Deseret', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,892 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansArabic-Bold.ttf', name='Noto Sans Arabic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,892 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansArabic-Bold.ttf', name='Noto Sans Arabic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,893 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMendeKikakui-Regular.ttf', name='Noto Sans Mende Kikakui', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,893 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMendeKikakui-Regular.ttf', name='Noto Sans Mende Kikakui', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,894 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTibetan-Regular.ttf', name='Noto Serif Tibetan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,894 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTibetan-Regular.ttf', name='Noto Serif Tibetan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,895 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedThaiUI-Regular.ttf', name='Noto Looped Thai UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,895 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedThaiUI-Regular.ttf', name='Noto Looped Thai UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,896 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGlagolitic-Regular.ttf', name='Noto Sans Glagolitic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,896 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGlagolitic-Regular.ttf', name='Noto Sans Glagolitic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,897 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMath-Regular.ttf', name='Noto Sans Math', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,897 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMath-Regular.ttf', name='Noto Sans Math', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,897 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTelugu-Regular.ttf', name='Noto Serif Telugu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,897 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTelugu-Regular.ttf', name='Noto Serif Telugu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,898 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansRejang-Regular.ttf', name='Noto Sans Rejang', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,898 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansRejang-Regular.ttf', name='Noto Sans Rejang', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,899 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/P052-Bold.otf', name='P052', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,899 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/P052-Bold.otf', name='P052', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,901 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifThai-Bold.ttf', name='Noto Serif Thai', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,901 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifThai-Bold.ttf', name='Noto Serif Thai', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,906 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOgham-Regular.ttf', name='Noto Sans Ogham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,906 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOgham-Regular.ttf', name='Noto Sans Ogham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,907 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDevanagari-Bold.ttf', name='Noto Serif Devanagari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,907 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDevanagari-Bold.ttf', name='Noto Serif Devanagari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,908 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Regular.otf', name='Nimbus Mono PS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,908 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Regular.otf', name='Nimbus Mono PS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,909 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf', name='Noto Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,909 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSans-Regular.ttf', name='Noto Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,910 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansChakma-Regular.ttf', name='Noto Sans Chakma', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,910 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansChakma-Regular.ttf', name='Noto Sans Chakma', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,911 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Main-Italic.ttf', name='KaTeX_Main', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,911 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Main-Italic.ttf', name='KaTeX_Main', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,912 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifNyiakengPuachueHmong-Regular.ttf', name='Noto Serif Hmong Nyiakeng', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,912 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifNyiakengPuachueHmong-Regular.ttf', name='Noto Serif Hmong Nyiakeng', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,913 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLaoUI-Bold.ttf', name='Noto Sans Lao UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,913 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLaoUI-Bold.ttf', name='Noto Sans Lao UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,914 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMalayalam-Bold.ttf', name='Noto Sans Malayalam', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,914 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMalayalam-Bold.ttf', name='Noto Sans Malayalam', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,915 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghHawad-Regular.ttf', name='Noto Sans Tifinagh Hawad', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,915 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghHawad-Regular.ttf', name='Noto Sans Tifinagh Hawad', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,916 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTangut-Regular.ttf', name='Noto Serif Tangut', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,916 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTangut-Regular.ttf', name='Noto Serif Tangut', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,917 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhmer-Bold.ttf', name='Noto Sans Khmer', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,917 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhmer-Bold.ttf', name='Noto Sans Khmer', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,923 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-RI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,923 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-RI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,924 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansInscriptionalPahlavi-Regular.ttf', name='Noto Sans Inscriptional Pahlavi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,924 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansInscriptionalPahlavi-Regular.ttf', name='Noto Sans Inscriptional Pahlavi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,926 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/URWBookman-Light.otf', name='URW Bookman', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-11-02 14:19:18,926 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/URWBookman-Light.otf', name='URW Bookman', style='normal', variant='normal', weight=300, stretch='normal', size='scalable')) = 10.145\n",
      "2025-11-02 14:19:18,927 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoTraditionalNushu-Regular.ttf', name='Noto Traditional Nushu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,927 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoTraditionalNushu-Regular.ttf', name='Noto Traditional Nushu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,928 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHebrew-Bold.ttf', name='Noto Sans Hebrew', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,928 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHebrew-Bold.ttf', name='Noto Sans Hebrew', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,929 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamil-Bold.ttf', name='Noto Serif Tamil', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,929 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamil-Bold.ttf', name='Noto Serif Tamil', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,930 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSans-Regular.otf', name='Nimbus Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,930 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSans-Regular.otf', name='Nimbus Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,932 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoNastaliqUrdu-Regular.ttf', name='Noto Nastaliq Urdu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,932 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoNastaliqUrdu-Regular.ttf', name='Noto Nastaliq Urdu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,933 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-Roman.otf', name='C059', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,933 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-Roman.otf', name='C059', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,933 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKannada-Regular.ttf', name='Noto Serif Kannada', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,933 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKannada-Regular.ttf', name='Noto Serif Kannada', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,934 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTeluguUI-Bold.ttf', name='Noto Sans Telugu UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,934 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTeluguUI-Bold.ttf', name='Noto Sans Telugu UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,936 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOldSouthArabian-Regular.ttf', name='Noto Sans Old South Arabian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,936 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOldSouthArabian-Regular.ttf', name='Noto Sans Old South Arabian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,937 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-Bold.otf', name='Nimbus Sans Narrow', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535\n",
      "2025-11-02 14:19:18,937 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-Bold.otf', name='Nimbus Sans Narrow', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 10.535\n",
      "2025-11-02 14:19:18,938 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,938 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-BoldItalic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,939 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSaurashtra-Regular.ttf', name='Noto Sans Saurashtra', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,939 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSaurashtra-Regular.ttf', name='Noto Sans Saurashtra', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,939 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/Z003-MediumItalic.otf', name='Z003', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145\n",
      "2025-11-02 14:19:18,939 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/Z003-MediumItalic.otf', name='Z003', style='italic', variant='normal', weight=500, stretch='normal', size='scalable')) = 11.145\n",
      "2025-11-02 14:19:18,940 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/P052-Roman.otf', name='P052', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,940 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/P052-Roman.otf', name='P052', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,942 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSoraSompeng-Bold.ttf', name='Noto Sans Sora Sompeng', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,942 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSoraSompeng-Bold.ttf', name='Noto Sans Sora Sompeng', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,942 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_SansSerif-Bold.ttf', name='KaTeX_SansSerif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,942 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_SansSerif-Bold.ttf', name='KaTeX_SansSerif', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,943 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifThai-Regular.ttf', name='Noto Serif Thai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,943 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifThai-Regular.ttf', name='Noto Serif Thai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,944 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifEthiopic-Regular.ttf', name='Noto Serif Ethiopic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,944 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifEthiopic-Regular.ttf', name='Noto Serif Ethiopic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,949 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGujarati-Regular.ttf', name='Noto Serif Gujarati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,949 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGujarati-Regular.ttf', name='Noto Serif Gujarati', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,950 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansAnatolianHieroglyphs-Regular.ttf', name='Noto Sans Anatolian Hieroglyphs', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,950 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansAnatolianHieroglyphs-Regular.ttf', name='Noto Sans Anatolian Hieroglyphs', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,951 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGurmukhi-Bold.ttf', name='Noto Serif Gurmukhi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,951 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGurmukhi-Bold.ttf', name='Noto Serif Gurmukhi', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,951 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansWancho-Regular.ttf', name='Noto Sans Wancho', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,951 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansWancho-Regular.ttf', name='Noto Sans Wancho', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,952 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOlChiki-Bold.ttf', name='Noto Sans Ol Chiki', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,952 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOlChiki-Bold.ttf', name='Noto Sans Ol Chiki', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,953 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGrantha-Regular.ttf', name='Noto Serif Grantha', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,953 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGrantha-Regular.ttf', name='Noto Serif Grantha', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,954 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHanifiRohingya-Bold.ttf', name='Noto Sans Hanifi Rohingya', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,954 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansHanifiRohingya-Bold.ttf', name='Noto Sans Hanifi Rohingya', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,954 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLisu-Bold.ttf', name='Noto Sans Lisu', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,954 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLisu-Bold.ttf', name='Noto Sans Lisu', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,955 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOldNorthArabian-Regular.ttf', name='Noto Sans Old North Arabian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,955 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOldNorthArabian-Regular.ttf', name='Noto Sans Old North Arabian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,956 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSoraSompeng-Regular.ttf', name='Noto Sans Sora Sompeng', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,956 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSoraSompeng-Regular.ttf', name='Noto Sans Sora Sompeng', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,956 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996\n",
      "2025-11-02 14:19:18,956 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 0.33499999999999996\n",
      "2025-11-02 14:19:18,961 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedLao-Regular.ttf', name='Noto Looped Lao', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,961 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedLao-Regular.ttf', name='Noto Looped Lao', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,962 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_AMS-Regular.ttf', name='KaTeX_AMS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,962 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_AMS-Regular.ttf', name='KaTeX_AMS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,963 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDuployan-Regular.ttf', name='Noto Sans Duployan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,963 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDuployan-Regular.ttf', name='Noto Sans Duployan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,964 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKhmer-Bold.ttf', name='Noto Serif Khmer', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,964 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKhmer-Bold.ttf', name='Noto Serif Khmer', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,965 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoRashiHebrew-Bold.ttf', name='Noto Rashi Hebrew', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,965 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoRashiHebrew-Bold.ttf', name='Noto Rashi Hebrew', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,965 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTakri-Regular.ttf', name='Noto Sans Takri', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,965 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTakri-Regular.ttf', name='Noto Sans Takri', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,966 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMasaramGondi-Regular.ttf', name='Noto Sans Masaram Gondi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,966 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMasaramGondi-Regular.ttf', name='Noto Sans Masaram Gondi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,967 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBassaVah-Regular.ttf', name='Noto Sans Bassa Vah', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,967 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBassaVah-Regular.ttf', name='Noto Sans Bassa Vah', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,968 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Size1-Regular.ttf', name='KaTeX_Size1', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,968 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Size1-Regular.ttf', name='KaTeX_Size1', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,969 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifHebrew-Bold.ttf', name='Noto Serif Hebrew', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,969 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifHebrew-Bold.ttf', name='Noto Serif Hebrew', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,974 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTamil-Bold.ttf', name='Noto Sans Tamil', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,974 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTamil-Bold.ttf', name='Noto Sans Tamil', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,975 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,975 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-Italic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,977 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDisplay-Regular.ttf', name='Noto Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,977 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDisplay-Regular.ttf', name='Noto Serif Display', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,978 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansJavanese-Regular.ttf', name='Noto Sans Javanese', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,978 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansJavanese-Regular.ttf', name='Noto Sans Javanese', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,979 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKharoshthi-Regular.ttf', name='Noto Sans Kharoshthi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,979 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKharoshthi-Regular.ttf', name='Noto Sans Kharoshthi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,980 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-Italic[wght].ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,980 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-Italic[wght].ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:18,983 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-Th.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,983 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-Th.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,984 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Size3-Regular.ttf', name='KaTeX_Size3', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,984 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Size3-Regular.ttf', name='KaTeX_Size3', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,984 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamilSlanted-Regular.ttf', name='Noto Serif Tamil Slanted', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,984 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTamilSlanted-Regular.ttf', name='Noto Serif Tamil Slanted', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,985 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,985 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSerif-BoldItalic.ttf', name='DejaVu Serif', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:18,986 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999\n",
      "2025-11-02 14:19:18,986 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed-Bold.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=700, stretch='condensed', size='scalable')) = 0.5349999999999999\n",
      "2025-11-02 14:19:18,987 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMeroitic-Regular.ttf', name='Noto Sans Meroitic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,987 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMeroitic-Regular.ttf', name='Noto Sans Meroitic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,988 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDogra-Regular.ttf', name='Noto Serif Dogra', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,988 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDogra-Regular.ttf', name='Noto Serif Dogra', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,989 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghAdrar-Regular.ttf', name='Noto Sans Tifinagh Adrar', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,989 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghAdrar-Regular.ttf', name='Noto Sans Tifinagh Adrar', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,989 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansThaana-Bold.ttf', name='Noto Sans Thaana', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,989 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansThaana-Bold.ttf', name='Noto Sans Thaana', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,990 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSymbols-Regular.ttf', name='Noto Sans Symbols', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,990 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSymbols-Regular.ttf', name='Noto Sans Symbols', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,995 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,995 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationMono-Bold.ttf', name='Liberation Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,996 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMro-Regular.ttf', name='Noto Sans Mro', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,996 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMro-Regular.ttf', name='Noto Sans Mro', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,997 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDevanagari-Bold.ttf', name='Noto Sans Devanagari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,997 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansDevanagari-Bold.ttf', name='Noto Sans Devanagari', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:18,998 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/D050000L.otf', name='D050000L', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,998 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/D050000L.otf', name='D050000L', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,999 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansNewTaiLue-Regular.ttf', name='Noto Sans New Tai Lue', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:18,999 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansNewTaiLue-Regular.ttf', name='Noto Sans New Tai Lue', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,000 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,000 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-M.ttf', name='Ubuntu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,000 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMalayalamUI-Regular.ttf', name='Noto Sans Malayalam UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,000 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMalayalamUI-Regular.ttf', name='Noto Sans Malayalam UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,001 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerif-Italic.ttf', name='Noto Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,001 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerif-Italic.ttf', name='Noto Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,002 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKannada-Bold.ttf', name='Noto Serif Kannada', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,002 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifKannada-Bold.ttf', name='Noto Serif Kannada', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,003 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25\n",
      "2025-11-02 14:19:19,003 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansCondensed.ttf', name='DejaVu Sans', style='normal', variant='normal', weight=400, stretch='condensed', size='scalable')) = 0.25\n",
      "2025-11-02 14:19:19,008 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKayahLi-Regular.ttf', name='Noto Sans Kayah Li', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,008 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKayahLi-Regular.ttf', name='Noto Sans Kayah Li', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,009 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSymbols2-Regular.ttf', name='Noto Sans Symbols2', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,009 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSymbols2-Regular.ttf', name='Noto Sans Symbols2', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,010 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifHebrew-Regular.ttf', name='Noto Serif Hebrew', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,010 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifHebrew-Regular.ttf', name='Noto Serif Hebrew', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,012 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBengali-Regular.ttf', name='Noto Sans Bengali', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,012 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBengali-Regular.ttf', name='Noto Sans Bengali', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,012 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerif-Regular.ttf', name='Noto Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,012 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerif-Regular.ttf', name='Noto Serif', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,013 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBengaliUI-Bold.ttf', name='Noto Sans Bengali UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,013 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBengaliUI-Bold.ttf', name='Noto Sans Bengali UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,014 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGurmukhi-Regular.ttf', name='Noto Serif Gurmukhi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,014 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifGurmukhi-Regular.ttf', name='Noto Serif Gurmukhi', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,015 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMedefaidrin-Bold.ttf', name='Noto Sans Medefaidrin', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,015 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMedefaidrin-Bold.ttf', name='Noto Sans Medefaidrin', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,016 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Main-Regular.ttf', name='KaTeX_Main', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,016 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Main-Regular.ttf', name='KaTeX_Main', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,016 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedLao-Bold.ttf', name='Noto Looped Lao', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,016 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedLao-Bold.ttf', name='Noto Looped Lao', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,017 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhmerUI-Regular.ttf', name='Noto Sans Khmer UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,017 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhmerUI-Regular.ttf', name='Noto Sans Khmer UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,022 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,022 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSerif-Italic.ttf', name='Liberation Serif', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,023 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansVai-Regular.ttf', name='Noto Sans Vai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,023 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansVai-Regular.ttf', name='Noto Sans Vai', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,024 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifBengali-Regular.ttf', name='Noto Serif Bengali', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,024 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifBengali-Regular.ttf', name='Noto Serif Bengali', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,025 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansArmenian-Regular.ttf', name='Noto Sans Armenian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,025 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansArmenian-Regular.ttf', name='Noto Sans Armenian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,026 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansPhoenician-Regular.ttf', name='Noto Sans Phoenician', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,026 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansPhoenician-Regular.ttf', name='Noto Sans Phoenician', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,026 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05\n",
      "2025-11-02 14:19:19,026 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSans-Oblique.ttf', name='DejaVu Sans', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 1.05\n",
      "2025-11-02 14:19:19,028 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhmer-Regular.ttf', name='Noto Sans Khmer', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,028 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansKhmer-Regular.ttf', name='Noto Sans Khmer', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,029 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Caligraphic-Regular.ttf', name='KaTeX_Caligraphic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,029 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/katex/KaTeX_Caligraphic-Regular.ttf', name='KaTeX_Caligraphic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,030 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGurmukhiUI-Bold.ttf', name='Noto Sans Gurmukhi UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,030 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGurmukhiUI-Bold.ttf', name='Noto Sans Gurmukhi UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,031 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBhaiksuki-Regular.ttf', name='Noto Sans Bhaiksuki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,031 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBhaiksuki-Regular.ttf', name='Noto Sans Bhaiksuki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,032 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGothic-Regular.ttf', name='Noto Sans Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,032 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGothic-Regular.ttf', name='Noto Sans Gothic', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,032 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghGhat-Regular.ttf', name='Noto Sans Tifinagh Ghat', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,032 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghGhat-Regular.ttf', name='Noto Sans Tifinagh Ghat', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,034 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMyanmarUI-Regular.ttf', name='Noto Sans Myanmar UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,034 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMyanmarUI-Regular.ttf', name='Noto Sans Myanmar UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,035 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDisplay-Bold.ttf', name='Noto Serif Display', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,035 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifDisplay-Bold.ttf', name='Noto Serif Display', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,036 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGujaratiUI-Regular.ttf', name='Noto Sans Gujarati UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,036 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGujaratiUI-Regular.ttf', name='Noto Sans Gujarati UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,037 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLydian-Regular.ttf', name='Noto Sans Lydian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,037 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLydian-Regular.ttf', name='Noto Sans Lydian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,038 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGeorgian-Bold.ttf', name='Noto Sans Georgian', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,038 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansGeorgian-Bold.ttf', name='Noto Sans Georgian', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,039 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-BoldOblique.otf', name='Nimbus Sans Narrow', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2025-11-02 14:19:19,039 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusSansNarrow-BoldOblique.otf', name='Nimbus Sans Narrow', style='oblique', variant='normal', weight=700, stretch='condensed', size='scalable')) = 11.535\n",
      "2025-11-02 14:19:19,040 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/URWBookman-LightItalic.otf', name='URW Bookman', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-11-02 14:19:19,040 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/URWBookman-LightItalic.otf', name='URW Bookman', style='italic', variant='normal', weight=300, stretch='normal', size='scalable')) = 11.145\n",
      "2025-11-02 14:19:19,040 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusRoman-Bold.otf', name='Nimbus Roman', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,040 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusRoman-Bold.otf', name='Nimbus Roman', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,041 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedLaoUI-Regular.ttf', name='Noto Looped Lao UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,041 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoLoopedLaoUI-Regular.ttf', name='Noto Looped Lao UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,042 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansElbasan-Regular.ttf', name='Noto Sans Elbasan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,042 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansElbasan-Regular.ttf', name='Noto Sans Elbasan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,047 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLao-Regular.ttf', name='Noto Sans Lao', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,047 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLao-Regular.ttf', name='Noto Sans Lao', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,048 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTelugu-Bold.ttf', name='Noto Serif Telugu', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,048 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifTelugu-Bold.ttf', name='Noto Serif Telugu', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,049 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTaiTham-Regular.ttf', name='Noto Sans Tai Tham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,049 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTaiTham-Regular.ttf', name='Noto Sans Tai Tham', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,050 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-LI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,050 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/Ubuntu-LI.ttf', name='Ubuntu', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,051 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMalayalam-Regular.ttf', name='Noto Sans Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,051 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMalayalam-Regular.ttf', name='Noto Sans Malayalam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,057 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMono-Bold.ttf', name='Noto Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,057 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMono-Bold.ttf', name='Noto Sans Mono', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,059 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansCuneiform-Regular.ttf', name='Noto Sans Cuneiform', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,059 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansCuneiform-Regular.ttf', name='Noto Sans Cuneiform', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,061 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Bold.otf', name='Nimbus Mono PS', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,061 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/NimbusMonoPS-Bold.otf', name='Nimbus Mono PS', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,062 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLao-Bold.ttf', name='Noto Sans Lao', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,062 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansLao-Bold.ttf', name='Noto Sans Lao', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,064 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-BdIta.otf', name='C059', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:19,064 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-BdIta.otf', name='C059', style='italic', variant='normal', weight=700, stretch='normal', size='scalable')) = 11.335\n",
      "2025-11-02 14:19:19,065 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOlChiki-Regular.ttf', name='Noto Sans Ol Chiki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,065 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOlChiki-Regular.ttf', name='Noto Sans Ol Chiki', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,071 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/URWGothic-BookOblique.otf', name='URW Gothic', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,071 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/URWGothic-BookOblique.otf', name='URW Gothic', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,073 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-Italic.otf', name='C059', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,073 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-Italic.otf', name='C059', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,075 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBuhid-Regular.ttf', name='Noto Sans Buhid', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,075 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansBuhid-Regular.ttf', name='Noto Sans Buhid', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,076 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTamilUI-Bold.ttf', name='Noto Sans Tamil UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,076 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTamilUI-Bold.ttf', name='Noto Sans Tamil UI', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,077 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSamaritan-Regular.ttf', name='Noto Sans Samaritan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,077 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansSamaritan-Regular.ttf', name='Noto Sans Samaritan', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,079 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghAhaggar-Regular.ttf', name='Noto Sans Tifinagh Ahaggar', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,079 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTifinaghAhaggar-Regular.ttf', name='Noto Sans Tifinagh Ahaggar', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,081 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansEthiopic-Bold.ttf', name='Noto Sans Ethiopic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,081 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansEthiopic-Bold.ttf', name='Noto Sans Ethiopic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,082 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,082 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', name='Liberation Sans', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,084 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,084 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/dejavu/DejaVuSansMono-Oblique.ttf', name='DejaVu Sans Mono', style='oblique', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,085 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansShavian-Regular.ttf', name='Noto Sans Shavian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,085 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansShavian-Regular.ttf', name='Noto Sans Shavian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,086 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMono-Regular.ttf', name='Noto Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,086 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMono-Regular.ttf', name='Noto Sans Mono', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,087 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-Bold.otf', name='C059', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,087 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/C059-Bold.otf', name='C059', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,087 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-BI.ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,087 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/ubuntu/UbuntuMono-BI.ttf', name='Ubuntu Mono', style='italic', variant='normal', weight=400, stretch='normal', size='scalable')) = 11.05\n",
      "2025-11-02 14:19:19,089 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansAdlam-Regular.ttf', name='Noto Sans Adlam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,089 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansAdlam-Regular.ttf', name='Noto Sans Adlam', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,090 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTelugu-Regular.ttf', name='Noto Sans Telugu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,090 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansTelugu-Regular.ttf', name='Noto Sans Telugu', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,091 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMarchen-Regular.ttf', name='Noto Sans Marchen', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,091 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMarchen-Regular.ttf', name='Noto Sans Marchen', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,091 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifMyanmar-Regular.ttf', name='Noto Serif Myanmar', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,091 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifMyanmar-Regular.ttf', name='Noto Serif Myanmar', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,092 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansThaiUI-Regular.ttf', name='Noto Sans Thai UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,092 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansThaiUI-Regular.ttf', name='Noto Sans Thai UI', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,093 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMeeteiMayek-Bold.ttf', name='Noto Sans Meetei Mayek', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,093 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansMeeteiMayek-Bold.ttf', name='Noto Sans Meetei Mayek', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,094 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOldPersian-Regular.ttf', name='Noto Sans Old Persian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,094 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSansOldPersian-Regular.ttf', name='Noto Sans Old Persian', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,101 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifEthiopic-Bold.ttf', name='Noto Serif Ethiopic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,101 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/truetype/noto/NotoSerifEthiopic-Bold.ttf', name='Noto Serif Ethiopic', style='normal', variant='normal', weight=700, stretch='normal', size='scalable')) = 10.335\n",
      "2025-11-02 14:19:19,102 - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/StandardSymbolsPS.otf', name='Standard Symbols PS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,102 - matplotlib.font_manager - DEBUG - findfont: score(FontEntry(fname='/usr/share/fonts/opentype/urw-base35/StandardSymbolsPS.otf', name='Standard Symbols PS', style='normal', variant='normal', weight=400, stretch='normal', size='scalable')) = 10.05\n",
      "2025-11-02 14:19:19,103 - DEBUG - findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/home/marcusp/miniconda3/lib/python3.12/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n",
      "2025-11-02 14:19:19,103 - matplotlib.font_manager - DEBUG - findfont: Matching sans\\-serif:style=normal:variant=normal:weight=normal:stretch=normal:size=12.0 to DejaVu Sans ('/home/marcusp/miniconda3/lib/python3.12/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf') with score of 0.050000.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import viz\n",
    "importlib.reload(viz)  # reloads the updated file\n",
    "\n",
    "\n",
    "viz.plot_curves(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd920e92-9ec9-4f4c-88b5-d0e04ffb3a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 14:36:07,059 - DEBUG - locator: <matplotlib.ticker.AutoLocator object at 0x72dbe0b30c20>\n",
      "2025-11-02 14:36:07,059 - matplotlib.colorbar - DEBUG - locator: <matplotlib.ticker.AutoLocator object at 0x72dbe0b30c20>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for batch_data, batch_labels in test_loader:\n",
    "    predictions = np.argmax(model.forward(batch_data), axis=1)\n",
    "    viz.plot_confusion_matrix(batch_labels, predictions, desired_classes)\n",
    "    break  # Just plot the first batch for demonstration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f8ffb-a29f-4bc8-b517-42d7fdabb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(viz)\n",
    "# Compute and plot saliency maps one sample at a time\n",
    "for batch_data, batch_labels in test_loader:\n",
    "    for i, input_sample in enumerate(batch_data):\n",
    "        input_sample = np.array(input_sample, dtype=float)\n",
    "        saliency_map = compute_saliency_map(model, input_sample, batch_size=1000)  # Use batch_size as needed\n",
    "        plot_saliency_map(saliency_map)\n",
    "\n",
    "        if i >= 3:  # just first 4 samples for demonstration\n",
    "            break\n",
    "    break  # first batch only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152be61-fc91-4cfb-9daf-7ac5572f61cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
